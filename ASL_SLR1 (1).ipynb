{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7cf524",
   "metadata": {},
   "source": [
    "# PROJECT STARTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5a983",
   "metadata": {},
   "source": [
    "# Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# import mediapipe as mp\n",
    "import csv\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89327d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-python\n",
    "# !pip3 install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3543af3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To save variable of the last session (avoid re-executing the cells)\n",
    "# dill.dump_session('base_variables3.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c5f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load session variables\n",
    "# dill.load_session('base_variables3.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195f4da",
   "metadata": {},
   "source": [
    "# Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path of dataset\n",
    "# video_path = r\"C:\\Users\\revan\\Downloads\\00335.mp4\"\n",
    "# folder_path = r'D:\\WLASL Datasets\\Kaggle_WLASL_withVideosInClassFolders\\dataset\\SL'\n",
    "folder_path = r'D:\\FYP_HWU\\Videos'\n",
    "DATA_PATH = r'D:\\FYP_HWU'\n",
    "# output_csv = os.path.join('dummy_dataset2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of actions to train model with (11 classes or actions)\n",
    "# actions = np.array(['Hello', 'Thank You', 'I Love You', 'Namaste'])\n",
    "actions = [\n",
    "'accident', \n",
    "'call', \n",
    "'help', \n",
    "'man', \n",
    "'murder', \n",
    "'woman', \n",
    "'danger',\n",
    "'police', \n",
    "'follow',\n",
    "'child',\n",
    "'sick'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c87b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_actions = ['follow', 'child','police']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472179a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae7098",
   "metadata": {},
   "source": [
    "### Counting number of videos under each action in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "video_count = {}\n",
    "\n",
    "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for i in dirs: #loop through each of the keywords or actions in the dataset\n",
    "        if (i in actions): #if the keyword is present in the shortlisted list of actions\n",
    "            for root, dirs, files in os.walk(os.path.join(folder_path, i)):\n",
    "                video_count[i] = len(dirs) \n",
    "                break\n",
    "    break\n",
    "\n",
    "print(video_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95624701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_count = {'accident': 13, 'call': 12, 'child': 9, 'danger': 11, 'help': 14, 'man': 12, 'murder': 13, 'police': 10, 'sick': 10, 'woman': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2524b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_video_count = {'follow': 36, 'child': 36, 'police': 40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "147cfdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_video_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f9e04",
   "metadata": {},
   "source": [
    "### Getting the video paths of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8065a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a dictionary for the actions and the count of videos for the chosen actions to train the model\n",
    "# for action in actions:\n",
    "#     classes[action] = my_dict[action]\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = {}\n",
    "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    \n",
    "    for action in actions:\n",
    "        for root, dirs, files in os.walk(os.path.join(folder_path, action)):\n",
    "            for i in range (len(files)):\n",
    "                files[i] = os.path.join(folder_path, action, files[i])\n",
    "            video_paths[action] = files\n",
    "            break\n",
    "    break\n",
    "\n",
    "# print(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_video_path = {}\n",
    "# temp_video_path['follow'] = video_paths['follow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4317688",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(temp_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0144d09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(video_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1285b",
   "metadata": {},
   "source": [
    "### Creating folders to store frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating folder 'Frames' that will contain all the video frames\n",
    "# os.makedirs(os.makedirs(os.path.join(DATA_PATH,'Frames')))\n",
    "# os.makedirs(os.path.join(DATA_PATH,'Original_Frames'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d648538",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(DATA_PATH,'Frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'D:\\FYP_HWU'\n",
    "DATA_PATH = os.path.join(DATA_PATH,'Rotate_Frames')\n",
    "\n",
    "#Creating one folder for each action\n",
    "for action in video_paths.keys():\n",
    "    # 1 folder for each video of the action\n",
    "    for sequence in range(len(video_paths[action])):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cd2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e300d2",
   "metadata": {},
   "source": [
    "## Extracting frames from videos and adding them to folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b725def",
   "metadata": {},
   "source": [
    "### APPROACH 1\n",
    "Here, a fixed number of frames are extracted from each video (60 in the current case). If the extracted frames from a video are less than 60, then the last frame is repeated to reach the 60 count. If the video is too long, then then the first 60 frames of the video will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c35c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #looping through each action\n",
    "# for action in video_paths.keys():\n",
    "    \n",
    "#     #Looping through each video of the action\n",
    "#     for sequence in range(len(video_paths[action])):\n",
    "        \n",
    "#         # Open the video file\n",
    "#         cap = cv2.VideoCapture(video_paths[action][sequence])\n",
    "        \n",
    "#         hc=[]\n",
    "        \n",
    "#         # Initializing variables\n",
    "#         frame_count = 0       # current frame count\n",
    "#         max_fc = 60          # maximum frame count (ranging from 65 to 70 frames)\n",
    "# #         DATA_PATH = os.path.join('Frames') #path to folder \"Frames\" to store the extracted frames\n",
    "        \n",
    "#         with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#             #while the video is accessible and the current frame count doesn't exceed the max frame count limit\n",
    "#             while (cap.isOpened() and frame_count < max_fc):\n",
    "#                 #reading the video frame\n",
    "#                 success, frame = cap.read()\n",
    "#                 #if there are frames\n",
    "#                 if success:\n",
    "#             #         print(\"SUCCESS\")\n",
    "#                     # Make detection\n",
    "#                     image, result = mediapipeHolistics(frame, holistic)\n",
    "\n",
    "#                     #Darwing landmarks on frames\n",
    "#                     draw_landmarks(image,result)\n",
    "\n",
    "#                     cv2.imshow(\"{} - {}\".format(action, sequence), image)\n",
    "                    \n",
    "#                     # saving the keypoints extracted (in numpy format)\n",
    "# #                     np.save(DATA_PATH, image)\n",
    "#                     keypoints = extract_keypoints(result)\n",
    "#                     npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "#                     np.save(npy_path, keypoints)\n",
    "                    \n",
    "#                     #saving the extracted frames (in jpg format)\n",
    "#                     cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "#                     frame_count+=1\n",
    "# #                     hc.append([join(DATA_PATH ,'/', action, '/', str(sequence), '/', 'frame'+ str(frame_count) + '.jpg'), action, frame_count])\n",
    "                    \n",
    "#                 else:\n",
    "#                     break\n",
    "                    \n",
    "#             # repeat last frame until we reach max frame count\n",
    "#             while frame_count < max_fc:\n",
    "#                 cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "#                 np.save(npy_path, keypoints)\n",
    "#                 frame_count+=1\n",
    "# #                     hc.append([join(DATA_PATH ,'/', action, '/', str(sequence), '/', 'frame'+ str(frame_count) + '.jpg'), action, frame_count])\n",
    "\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7761ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through each action\n",
    "for action in video_paths.keys():\n",
    "    \n",
    "    #Looping through each video of the action\n",
    "    for sequence in range(len(video_paths[action])):\n",
    "        \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_paths[action][sequence])\n",
    "        \n",
    "        # Initializing variables\n",
    "        frame_count = 0       # current frame count\n",
    "        max_fc = 60          # maximum frame count\n",
    "        \n",
    "\n",
    "        #while the video is accessible and the current frame count doesn't exceed the max frame count limit\n",
    "        while (cap.isOpened() and frame_count < max_fc):\n",
    "            #reading the video frame\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            #if there are frames\n",
    "            if success:\n",
    "                image = frame.copy()\n",
    "                cv2.imshow(\"{} - {}\".format(action, sequence), image)\n",
    "                \n",
    "                #saving the extracted frames (in jpg format)\n",
    "                cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "                frame_count+=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # repeat last frame until we reach max frame count\n",
    "        while frame_count < max_fc:\n",
    "            cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "            frame_count+=1\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4526c39",
   "metadata": {},
   "source": [
    "### APPROACH 2\n",
    "Here too, a fixed number of frames are extracted from each video (25/30 in the current case). The frames will be extracted after dividing the complete video into equal parts and then choosing the frames from each part. This way we will be able to avoid the first few or last frames few of the video if no action is being depicted. Secondly, the basic movement of the action is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescaleFrame(inputBatch, scaleFactor = 50):\n",
    "    ''' returns constant frames for any length video \n",
    "        scaleFactor: number of constant frames to get.  \n",
    "        inputBatch : frames present in the video. \n",
    "    '''\n",
    "    if len(inputBatch) < 1:\n",
    "        return\n",
    "    \"\"\" This is to rescale the frames to specific length considering almost all the data in the batch  \"\"\" \n",
    "    skipFactor = len(inputBatch)//scaleFactor\n",
    "    return [inputBatch[i] for i in range(0, len(inputBatch), skipFactor)][:scaleFactor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' read the frames from the video '''\n",
    "frames = []\n",
    "cap = cv2.VideoCapture('sample.mp4')\n",
    "ret, frame = cap.read()\n",
    "while True:\n",
    "    if not ret:\n",
    "        print('no frames')\n",
    "        break\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' to get the constant frames from varying number of frames, call the rescaleFrame() function '''\n",
    "outputFrames = rescaleFrames(inputBatch=frames, scaleFactor = 45) # these are the output frames for 1 video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a303a8",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae365",
   "metadata": {},
   "source": [
    "By the end of this section, we will folders containing the images frames stored in jpg format for each video of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944047ec",
   "metadata": {},
   "source": [
    "## Data Augmentation \n",
    " Using:\n",
    " - Frame Mirroring \n",
    " - Changing contrast and brightness\n",
    " - Rotating \n",
    " \n",
    " Here, we take each video, extract frames and for each frame, apply the filters (which may wither include changing the contrast and brightness of the image frame, mirroring the image or extracting only the keypoints and the edges from the image), and store them as a separate video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11847f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackAndWhite(image): # takes image as a parameter\n",
    "    # Image color\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    new_image = enhancer.enhance(0)  \n",
    "    \n",
    "    # return np.array(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saturation(image): # takes image as a parameter\n",
    "    # Horizontally flipping the image\n",
    "    image = flipImage(image)\n",
    "    # Image color\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    new_image = enhancer.enhance(1.5)  \n",
    "    \n",
    "    # return np.array(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipImage(image): # takes image as a parameter\n",
    "    # Converting Image to numpy array\n",
    "    new_image = np.array(image)\n",
    "    \n",
    "    # Horizontally flipping the image\n",
    "    image = cv2.flip(new_image, 1)\n",
    "    \n",
    "    # Converting numpy aray to image format\n",
    "    image = Image.fromarray(image.astype('uint8'))  \n",
    "    \n",
    "    # Returning the image in image format\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateImage(image):\n",
    "    image  = image.rotate(-10)  #- or + -> left or right\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547ff254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the final number of videos under each Action \n",
    "def totalVideoCount(DATA_PATH):\n",
    "    finalVideoCount = {}\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for folder in dirs:\n",
    "            for root, dirs, files in os.walk(os.path.join(DATA_PATH, folder)):\n",
    "                finalVideoCount[folder] = len(dirs)\n",
    "                break\n",
    "    return finalVideoCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "videoCount = totalVideoCount(PATH)\n",
    "print(videoCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = os.path.join(DATA_PATH,'accident', '6', 'frame' + '11' +'.jpg')\n",
    "# print(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0549ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize = (224,224)\n",
    "# img = cv2.imread(IMAGE_PATH)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# # transposed  = img.rotate(-10) \n",
    "# # new_image = np.array(transposed)\n",
    "# new_image = crop_center_square(img)\n",
    "# new_image = cv2.resize(new_image, resize)\n",
    "# image = Image.fromarray(new_image.astype('uint8')) \n",
    "# new_image = rotateImage(image)\n",
    "# new_image.show()\n",
    "# # image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073adf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_count = dictionary that maps actions to the number of videos for that action\n",
    "# video_path = dictionary that maps actions to a list containing the path of videos under that action\n",
    "# max_fc = 60 (maximum number of frames extracted per video), variable already initialized earlier\n",
    "\n",
    "# loop through each video (sequence) for each action\n",
    "# create another sequece folder (my_dict[action] + count), where count is incremented with every sequence loop iteration\n",
    "# for loop for i in range(60):\n",
    "# get the image -> concat('frame', str(i))\n",
    "# apply filters\n",
    "# save the image in the new folder created\n",
    "\n",
    "max_fc = 60\n",
    "AUG_PATH = r'D:\\FYP_HWU\\Aug_Frames'\n",
    "PATH = r'D:\\FYP_HWU\\Rotate_Frames'\n",
    "DATA_PATH = r'D:\\FYP_HWU\\Frames'\n",
    "\n",
    "finalVideoCount = totalVideoCount(AUG_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in video_count.keys():\n",
    "    actionCount = finalVideoCount[action]\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(video_count[action]):\n",
    "        \n",
    "        #creating a folder to store the augmented images\n",
    "        folder_number = actionCount + sequence\n",
    "        if(folder_number < 50):\n",
    "            try:\n",
    "                os.makedirs(os.path.join(PATH, action, str(folder_number)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for frame_num in range (max_fc): #max frames = 60\n",
    "                IMAGE_PATH = os.path.join(DATA_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "                # Reading the image\n",
    "                image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "                #Converting numpy array to Image\n",
    "                image = Image.fromarray(image.astype('uint8')) \n",
    "\n",
    "                # Applying the filters\n",
    "#                 new_image = blackAndWhite(image)\n",
    "#                 new_image = flipImage(image)\n",
    "#                 new_image = saturation(image)\n",
    "                new_image = rotateImage(image)\n",
    "\n",
    "                # Converting Image to numpy array\n",
    "                new_image = np.array(new_image)\n",
    "\n",
    "                #Saving the image in the folder created\n",
    "                cv2.imwrite(os.path.join(PATH , str(action), str(folder_number), 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
    "\n",
    "                # Displaying Image\n",
    "                cv2.destroyAllWindows()\n",
    "        \n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f661404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bac943",
   "metadata": {},
   "source": [
    "## Resizing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a99f38",
   "metadata": {},
   "source": [
    "Resizing the images to 224 x 224 pixel size to feed to ResNet 50 pre trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e264f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping the center of the image (cropping out the extra background margins of the video.)\n",
    "def crop_center_square(frame): # takes image as a parameter\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize = (224,224)\n",
    "# IMAGE_PATH = r'C:\\Users\\revan\\Downloads\\frame3.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(IMAGE_PATH)\n",
    "# image = Image.fromarray(image.astype('uint8')) \n",
    "# # image.show()\n",
    "\n",
    "# new_image = np.array(image)\n",
    "# new_image = crop_center_square(new_image)\n",
    "# new_image = cv2.resize(new_image, resize)\n",
    "# image = Image.fromarray(new_image.astype('uint8')) \n",
    "# image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d53127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize the frames\n",
    "def resizeFrames(new_image, resize): # takes numpy array as parameter\n",
    "    new_image = crop_center_square(new_image)\n",
    "    new_image = cv2.resize(new_image, resize)\n",
    "    return new_image # returns a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fc = 60\n",
    "resize = (224,224)\n",
    "AUG_PATH = r'D:\\FYP_HWU\\Aug_Frames'\n",
    "FRAMES_PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "\n",
    "finalVideoCount = totalVideoCount(AUG_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in finalVideoCount.keys():\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(finalVideoCount[action]):\n",
    "\n",
    "        try:\n",
    "            os.makedirs(os.path.join(FRAMES_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for frame_num in range (max_fc): #max frames = 60\n",
    "            IMAGE_PATH = os.path.join(AUG_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "            # Reading the image\n",
    "            image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "            new_image = resizeFrames(image, resize)\n",
    "\n",
    "            #Saving the image in the folder created\n",
    "            cv2.imwrite(os.path.join(FRAMES_PATH , str(action), str(sequence), 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
    "\n",
    "            # Displaying Image\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f20f4",
   "metadata": {},
   "source": [
    "## Keypoint Extraction using MediaPipe Holistics\n",
    "Applying MediaPipe Keypoint Landmarks to the extracted frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b61ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #Holistic model (use for detections)\n",
    "mp_drawing = mp.solutions.drawing_utils #Drawing Utilities\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness = 1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4039ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capturing landmarks\n",
    "def mediapipeHolistics(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion -> BGR to RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) # Making Prediction \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion -> RGB to BGR\n",
    "    return image, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbe39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize landmarks, connecting the landmarks on the image (drawing only pose and had landmarks)\n",
    "def draw_landmarks(image, results):\n",
    "\n",
    "#     mp_drawing.draw_landmarks(image = image, \n",
    "#                               landmark_list = results.face_landmarks, \n",
    "#                               connections = mp_holistic.FACEMESH_CONTOURS,\n",
    "#                               landmark_drawing_spec = drawing_spec,\n",
    "#                               connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.pose_landmarks, \n",
    "                              connections = mp_holistic.POSE_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.left_hand_landmarks, \n",
    "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.right_hand_landmarks, \n",
    "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing hand, pose and face landmarks\n",
    "def extract_keypoints(result):\n",
    "    pose = np.array([[res.x,res.y,res.z,res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4)\n",
    "    leftHand = np.array([[res.x,res.y,res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    rightHand = np.array([[res.x,res.y,res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    face = np.array([[res.x,res.y,res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose, face, leftHand, rightHand])\n",
    "#     return np.concatenate([pose, leftHand, rightHand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = os.path.join(FRAMES_PATH,'accident', '8', 'frame' + '30' +'.jpg')\n",
    "# print(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25501cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resize = (224,224)\n",
    "# img = cv2.imread(IMAGE_PATH)\n",
    "# with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#     # Making detection\n",
    "#     image, result = mediapipeHolistics(img, holistic)\n",
    "    \n",
    "#     #Darwing landmarks on frames\n",
    "#     draw_landmarks(image,result)\n",
    "\n",
    "# # #Saving the keypoints extracted\n",
    "# keypoints = extract_keypoints(result)    \n",
    "# print(len(keypoints))\n",
    "# image = Image.fromarray(image.astype('uint8')) \n",
    "# image.show()\n",
    "# # image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0907fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fc = 60\n",
    "RESIZE_PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "FRAMES_PATH = r'D:\\FYP_HWU\\MP_Frames'\n",
    "NP_PATH = r'D:\\FYP_HWU\\MP_npArray'\n",
    "\n",
    "finalVideoCount = totalVideoCount(RESIZE_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in finalVideoCount.keys():\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(finalVideoCount[action]):\n",
    "        \n",
    "        # Creating folders to store frames and extracted landmarks\n",
    "        try:\n",
    "            os.makedirs(os.path.join(FRAMES_PATH, action, str(sequence)))\n",
    "            os.makedirs(os.path.join(NP_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "            for frame_num in range (max_fc): #max frames = 60\n",
    "                \n",
    "                # Getting the extracted frame from the folder\n",
    "                IMAGE_PATH = os.path.join(RESIZE_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "                # Reading the image\n",
    "                image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "                # Making detection\n",
    "                image, result = mediapipeHolistics(image, holistic)\n",
    "\n",
    "                #Darwing landmarks on frames\n",
    "                draw_landmarks(image,result)\n",
    "\n",
    "                #Saving the keypoints extracted\n",
    "                keypoints = extract_keypoints(result) \n",
    "                \n",
    "                #Storing keypoints in numpy format\n",
    "                npy_path = os.path.join(NP_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "#                 image = Image.fromarray(image.astype('uint8')) \n",
    "\n",
    "                #Saving the image in the folder created\n",
    "                cv2.imwrite(os.path.join(FRAMES_PATH , str(action), str(sequence), 'frame'+ str(frame_num) + '.jpg'), image)\n",
    "\n",
    "                # Displaying Image\n",
    "                cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21b136",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1b3b8",
   "metadata": {},
   "source": [
    "### Visualizing dataset frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74910e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# man (video 0, frame 22)\n",
    "img_man = image.load_img(\"{}/man/0/frame22.jpg\". format(DATA_PATH))\n",
    "print(\"ACTION: Man    Video: 0     Frame: 22\")\n",
    "img_man"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3d374",
   "metadata": {},
   "source": [
    "## Creating Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03592f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting data in 80:10:10 train:validation:test ratio\n",
    "# def trainTestSplit(X,y):\n",
    "#     x_train, x_temp, y_train, y_temp= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.5, random_state=42)\n",
    "#     return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f12045ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(X,y):\n",
    "    x_train, x_temp, y_train, y_temp= train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    return x_train, y_train, x_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dc733",
   "metadata": {},
   "source": [
    "### Preparing Data with 'Mediapipe Landmarks' stored as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b274e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641b9556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'police': 40, 'follow': 36, 'child': 36}\n"
     ]
    }
   ],
   "source": [
    "# Creating subset to to test whether the model works fine\n",
    "total_video_count = totalVideoCount(DATA_PATH)\n",
    "temp_list = ['police', 'follow', 'child']\n",
    "subset_video_count = {}\n",
    "for temp in temp_list:\n",
    "    subset_video_count[temp] = total_video_count[temp]\n",
    "    \n",
    "print(subset_video_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f3609ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_video_count = {'police': 40, 'follow': 36}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f88d74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting numeric labels for non numeric action category values\n",
    "# label_map = {label:num for num, label in enumerate(video_count.keys())} # all classes\n",
    "label_map = {label:num for num, label in enumerate(subset_video_count.keys())} #subset of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29263b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'police': 0, 'follow': 1}\n"
     ]
    }
   ],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33039bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_npArray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798440c4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUBSET CLASSES\n",
    "\n",
    "#features = non target columns    labels = target columns\n",
    "\n",
    "max_fc = 60\n",
    "features, labels = [], []\n",
    "# Iterating through each action\n",
    "for action in total_video_count.keys():\n",
    "    \n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(total_video_count[action]):\n",
    "        \n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        \n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            \n",
    "            # Declaring directory of the keypoints\n",
    "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            \n",
    "            # Loading the numpy array\n",
    "            res = np.load(IMAGE_PATH)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(res)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        features.append(frames)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL CLASSES\n",
    "\n",
    "max_fc = 60\n",
    "features, labels = [], []\n",
    "# Iterating through each action\n",
    "for action in subset_video_count.keys():\n",
    "    \n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(subset_video_count[action]):\n",
    "        \n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        \n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            \n",
    "            # Declaring directory of the keypoints\n",
    "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            \n",
    "            # Loading the numpy array\n",
    "            res = np.load(IMAGE_PATH)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(res)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        features.append(frames)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(features).shape\n",
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62390ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(labels).shape\n",
    "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1] \n",
    "y = to_categorical(labels).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6512f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train, Validation and Test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163062cb",
   "metadata": {},
   "source": [
    "### Preparing Data with Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a52b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Computer Paths\n",
    "IMAGE_PATH = r'C:\\Users\\revan\\Downloads\\frame101.jpg'\n",
    "DATA_PATH = r'C:\\Users\\revan\\Downloads\\BOOK.mp4'\n",
    "IM_PATH = r'C:\\Users\\revan\\Downloads\\frame101.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0557ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cap = cv2.VideoCapture(DATA_PATH)\n",
    "# # # DATAPATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "# frame = cv2.imread(IMAGE_PATH)\n",
    "# # frame = frame[:, :, [2, 1, 0]]\n",
    "# image = Image.fromarray(frame.astype('uint8')) \n",
    "# # new_image := np.array(image)\n",
    "# # cv2.imwrite(IM_PATH,new_image)\n",
    "# image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the video count\n",
    "# video_count = {'accident': 40, 'call': 40, 'child': 36, 'danger': 40, 'follow': 36, 'help': 40, 'man': 40, 'murder': 40, 'police': 40, 'sick': 40, 'woman': 40}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea605fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# University Computer Paths\n",
    "\n",
    "# IMAGE_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Frames\\accident\\0\\frame0.jpg'\n",
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'\n",
    "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12974db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameEnhance(frame):\n",
    "    # Converting BGR -> RGB\n",
    "    frame = frame[:, :, [2, 1, 0]]         \n",
    "    # Normalize the pixel values\n",
    "    frame = frame / 255.0\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm as tqdm\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# values = range(3)\n",
    "# with tqdm(total=len(values)) as pbar:\n",
    "#     for i in values:\n",
    "#         pbar.write('processed: %d' %i)\n",
    "#         pbar.update(1)\n",
    "#         sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all classes\n",
    "max_fc = 60\n",
    "videoFrames=[]\n",
    "videolabels=[]\n",
    "# Iterating through each action\n",
    "for action in video_count.keys():\n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(video_count[action]): # video_count[action]\n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            # Loading the image frame\n",
    "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
    "            \n",
    "            # # Converting BGR -> RGB\n",
    "            # frame = frame[:, :, [2, 1, 0]]\n",
    "            \n",
    "            # # Normalize the pixel values\n",
    "            # frame = frame / 255.0\n",
    "\n",
    "            frame = frameEnhance(frame)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(frame)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        videoFrames.append(frames)\n",
    "        videolabels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e369980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Resized_Frames'\n",
    "DATA_PATH = r'D:\\FYP_HWU\\MP_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = np.array([253,244,243])\n",
    "# array = array/255.0\n",
    "# print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4511c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'police': 40, 'follow': 36}\n",
      "D:\\FYP_HWU\\MP_Frames\n"
     ]
    }
   ],
   "source": [
    "print(subset_video_count)\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bde04ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action police done\n",
      "Action follow done\n"
     ]
    }
   ],
   "source": [
    "# For subset of the classes (3 classes) [child, follow, police]\n",
    "max_fc = 60\n",
    "videoFrames=[]\n",
    "videolabels=[]\n",
    "# Iterating through each action\n",
    "for action in subset_video_count.keys():\n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(subset_video_count[action]):\n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        # Iterating through each frame\n",
    "        for frame_num in range(max_fc):\n",
    "            # Loading the image frame\n",
    "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
    "            \n",
    "            # # Converting BGR -> RGB\n",
    "            # frame = frame[:, :, [2, 1, 0]]\n",
    "            \n",
    "            # # Normalize the pixel values\n",
    "            # frame = frame / 255.0\n",
    "            frame = frameEnhance(frame)\n",
    "            \n",
    "#             videoFrames.append(frame)\n",
    "#             videolabels.append(label_map[action])\n",
    "            # Appending to the list\n",
    "            frames.append(frame)\n",
    "            cv2.destroyAllWindows()\n",
    "        # Adding to the dataset\n",
    "        videoFrames.append(frames)\n",
    "        videolabels.append(label_map[action])\n",
    "\n",
    "    print(\"Action {} done\".format(action)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26600d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action child done\n"
     ]
    }
   ],
   "source": [
    "# # For subset of the classes (3 classes) [child, follow, police]\n",
    "# max_fc = 60\n",
    "# videoFrames=[]\n",
    "# videolabels=[]\n",
    "# # Iterating through each action\n",
    "# for action in subset_video_count.keys():\n",
    "#     # Iterting through each video in the action\n",
    "#     for sequence in range(subset_video_count[action]):\n",
    "#         # Iterating through each frame\n",
    "#         for frame_num in range(max_fc):\n",
    "#             # Loading the image frame\n",
    "#             frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
    "            \n",
    "#             # # Normalize the pixel values\n",
    "#             frame = frameEnhance(frame)\n",
    "#             img_array_flat = frame.flatten()\n",
    "            \n",
    "#             frame = img_array_flat.reshape(-1, 1).T\n",
    "            \n",
    "#             # Adding to the dataset\n",
    "#             videoFrames.append(frame)\n",
    "#             videolabels.append(label_map[action])\n",
    "#             cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#     print(\"Action {} done\".format(action)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e83168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(videoFrames).shape\n",
    "X = np.array(videoFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26505e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "print(len(videoFrames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03dbe16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'D:\\FYP_HWU\\X_data_sub.csv'\n",
    "# np.savetxt(path, X, delimiter=',', fmt='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2ca6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8116201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path where you want to save the HDF5 file\n",
    "# hdf5_file_path = 'X_data.h5'\n",
    "\n",
    "# # Write the data to the HDF5 file\n",
    "# with h5py.File(hdf5_file_path, 'w') as hdf5_file:\n",
    "#     hdf5_file.create_dataset('X_data', data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62c35324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('X_data.h5', 'r') as hdf5_file:\n",
    "#     restored_data = hdf5_file['X_data'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f15131",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'restored_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrestored_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'restored_data' is not defined"
     ]
    }
   ],
   "source": [
    "# restored_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54813e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(videolabels).shape\n",
    "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1] \n",
    "y = to_categorical(videolabels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95a7730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "447dbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_file_path = 'y_data.h5'\n",
    "\n",
    "# # Write the data to the HDF5 file\n",
    "# with h5py.File(hdf5_file_path, 'w') as hdf5_file:\n",
    "#     hdf5_file.create_dataset('y_data', data=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab390bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('y_data.h5', 'r') as hdf5_file:\n",
    "#     restored_data = hdf5_file['y_data'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restored_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3062780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train, Validation and Test\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)\n",
    "X_train, y_train, X_val, y_val = trainTestSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eae675e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8de003a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5860b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b2e1b",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b3c77",
   "metadata": {},
   "source": [
    "## Building model for MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0a4d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# # image processing\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "# from keras.preprocessing import sequence\n",
    "\n",
    "# model / neural network\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d9e9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a763e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "# frames = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1caebf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = r'D:\\FYP_HWU\\Logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fa044f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283299f0",
   "metadata": {},
   "source": [
    "RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ecf297a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs\\ckpt.RES50Bi_model.h5\n"
     ]
    }
   ],
   "source": [
    "# folder_path = os.path.join(log_dir,\"ckpt.RES50Bi_model.h5\")\n",
    "folder_path = os.path.join(log_dir,\"ckpt.CNNBi_model.keras\")\n",
    "print(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e8484ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "# checkpoint = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "folder_path = os.path.join(log_dir,\"ckpt.RES50Bi_model.h5\")\n",
    "print(folder_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        folder_path, save_best_only=True, monitor = \"val_accuracy\", mode = \"max\"\n",
    "    )\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70157764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee21ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59994c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b1a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d38d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the entire model\n",
    "# x = resnet_50.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(512, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(64, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# predictions = layers.Dense(5, activation='softmax')(x)\n",
    "# model = Model(inputs = resnet_50.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51bd75",
   "metadata": {},
   "source": [
    "BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7859fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  history=model.fit(x_train, y_train,\n",
    "#            batch_size=batch_size,\n",
    "#            epochs=20,\n",
    "#            validation_data=[x_test, y_test])\n",
    "#  print(history.history['loss'])\n",
    "#  print(history.history['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a57dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model \n",
    "# model = tf.keras.Sequential([ \n",
    "#     encoder, \n",
    "#     tf.keras.layers.Embedding(11, 64, mask_zero=True), \n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), \n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \n",
    "#     tf.keras.layers.Dense(64, activation='relu'), \n",
    "#     tf.keras.layers.Dense(1) \n",
    "# ]) \n",
    "  \n",
    "# # Compile the model \n",
    "# model.compile( \n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "#     optimizer=tf.keras.optimizers.Adam(), \n",
    "#     metrics=['accuracy'] \n",
    "# )\n",
    "\n",
    "\n",
    "# # Training the model and validating it on test set \n",
    "# history = model.fit( \n",
    "#     train_dataset,  \n",
    "#     epochs=5, \n",
    "#     validation_data=test_dataset, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d571e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu')))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_corssentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=20, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bilstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea75b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install --user numpy==1.22.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7e79b",
   "metadata": {},
   "source": [
    "# #-------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74368859",
   "metadata": {},
   "source": [
    "RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet50 model\n",
    "# resnet_50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "# for layer in resnet_50.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the entire model\n",
    "# x = resnet_50.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(512, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(64, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# predictions = layers.Dense(5, activation='softmax')(x)\n",
    "# model = Model(inputs = resnet_50.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0b438",
   "metadata": {},
   "source": [
    "3D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import h5py\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.initializers import Constant\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b657a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(layers.Conv3D(32,(3,3,3),activation='relu',input_shape=(16,16,16,1),bias_initializer=Constant(0.01)))\n",
    "# model.add(layers.Conv3D(32,(3,3,3),activation='relu',bias_initializer=Constant(0.01)))\n",
    "# model.add(layers.MaxPooling3D((2,2,2)))\n",
    "# model.add(layers.Conv3D(64,(3,3,3),activation='relu'))\n",
    "# model.add(layers.Conv3D(64,(2,2,2),activation='relu'))\n",
    "# model.add(layers.MaxPooling3D((2,2,2)))\n",
    "# model.add(layers.Dropout(0.6))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(256,'relu'))\n",
    "# model.add(layers.Dropout(0.7))\n",
    "# model.add(layers.Dense(128,'relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.Dense(10,'softmax'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5949e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(Adam(0.001),'categorical_crossentropy',['accuracy'])\n",
    "# model.fit(xtrain,ytrain,epochs=200,batch_size=32,verbose=1,validation_data=(xtest,ytest),callbacks=[EarlyStopping(patience=15)])\n",
    "# Testing the 3D-CNN\n",
    "# _, acc = model.evaluate(xtrain, ytrain)\n",
    "# print('training accuracy:', str(round(acc*100, 2))+'%')\n",
    "# _, acc = model.evaluate(xtest, ytest)\n",
    "# print('testing accuracy:', str(round(acc*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe6be3",
   "metadata": {},
   "source": [
    "TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e66322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# # Define the number of layers, heads, and dimensions of the model\n",
    "# num_layers = 6\n",
    "# num_heads = 8\n",
    "# d_model = 512\n",
    "# # Define the encoder and decoder layers\n",
    "# encoder_layers = [\n",
    "#     keras.layers.MultiHeadAttention(num_heads, d_model),\n",
    "#     keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Add(),\n",
    "#     keras.layers.LayerNormalization()\n",
    "# ]\n",
    "# decoder_layers = [\n",
    "#     keras.layers.MultiHeadAttention(num_heads, d_model),\n",
    "#     keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Add(),\n",
    "#     keras.layers.LayerNormalization()\n",
    "# ]\n",
    "# # Create the encoder and decoder\n",
    "# encoder = keras.layers.Encoder(encoder_layers, num_layers)\n",
    "# decoder = keras.layers.Decoder(decoder_layers, num_layers)\n",
    "# # Define the final model\n",
    "# model = keras.models.Transformer(encoder, decoder)\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = keras.optimizers.Adam()\n",
    "# loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "# # Train the model on your dataset\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f91f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e5d787",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4d55ffb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs\n"
     ]
    }
   ],
   "source": [
    "# log_dir = r'D:\\FYP_HWU\\Logs'\n",
    "# os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b7a3f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "# checkpoint = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "log_dir = r'D:\\FYP_HWU\\Logs'\n",
    "folder_path = os.path.join(log_dir,\"ckpt.CNNBiLSTM_sub.keras\")\n",
    "# folder_path = os.path.join(log_dir,\"ckpt.Res50BiLSTM_sub.keras\")\n",
    "# print(folder_path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        folder_path, save_best_only=True, monitor = \"val_accuracy\", mode = \"max\"\n",
    "    )\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0767fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Dropout, LSTM, Bidirectional, Input, concatenate, GlobalAveragePooling2D, GlobalMaxPool2D, Reshape\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6e189ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "actions = len(subset_video_count.keys())\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d957cb8",
   "metadata": {},
   "source": [
    "RESNET 50 + BiLSTM (224 x 224 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5eb139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 60, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet_model.trainable = False  # Freeze the weights of the ResNet50 model\n",
    "\n",
    "noActions = len(subset_video_count.keys())\n",
    "\n",
    "# Define the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "# Additional layers\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(32, activation='relu'))\n",
    "bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
    "\n",
    "# Combine the ResNet50 and BiLSTM models\n",
    "combined_input = Input(shape=(60, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
    "resnet_output = TimeDistributed(resnet_model)(combined_input)\n",
    "resnet_output = TimeDistributed(GlobalAveragePooling2D())(resnet_output)  # Adjust pooling layer based on your requirements\n",
    "# resnet_output = Reshape((60, -1))(resnet_output)\n",
    "# print(resnet_output.shape)\n",
    "bilstm_output = bilstm_model(resnet_output)\n",
    "\n",
    "# ---------------\n",
    "\n",
    "\n",
    "# Concatenate the outputs\n",
    "# merged = concatenate([resnet_output, bilstm_output])\n",
    "\n",
    "# Additional layers for combined model\n",
    "# final_output = Dense(128, activation='relu')(merged)\n",
    "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
    "\n",
    "# Create the final model\n",
    "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
    "\n",
    "# ----------------\n",
    "hybrid_model1 = Model(inputs=combined_input, outputs=bilstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5255150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_renet50(shape=(224, 224, 3)):\n",
    "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    resnet_model.trainable = False  # Freeze the weights of the ResNet50 model\n",
    "    output = GlobalMaxPool2D()\n",
    "    return Sequential([resnet_model, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bf724f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Res50_BiLSTM(shape=(60, 224, 224, 3), noActions=len(subset_video_count.keys())):\n",
    "    resnet_model = build_renet50(shape[1:])\n",
    "    \n",
    "    # Define the BiLSTM model\n",
    "    bilstm_model = Sequential()\n",
    "    bilstm_model.add(TimeDistributed(resnet_model, input_shape=shape))\n",
    "    # bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "    bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "    bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "    # bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "    # Additional layers\n",
    "    bilstm_model.add(Dense(64, activation='relu'))\n",
    "    bilstm_model.add(Dropout(.5))\n",
    "    bilstm_model.add(Dense(32, activation='relu'))\n",
    "    bilstm_model.add(Dropout(.5))\n",
    "    bilstm_model.add(Dense(32, activation='relu'))\n",
    "    bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
    "\n",
    "    return bilstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7d241830",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model1 = Res50_BiLSTM()\n",
    "# optimizer = keras.optimizers.SGD()\n",
    "# hybrid_model1_1.compile(\n",
    "#     optimizer,\n",
    "#     'categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2b9a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\revan\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "hybrid_model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "113ebabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# frames = 60\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db41abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 163s 5s/step - loss: 1.4542 - accuracy: 0.4118 - val_loss: 0.7121 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# history1  = hybrid_model1.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly\n",
    "history1_2  = hybrid_model1_1.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08074a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_model1.save('RES50BiModel_sub.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18416963",
   "metadata": {},
   "source": [
    "CNN + BiLSTM (224 x 224 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3d4d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aafca8d0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m combined_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# Assuming image shape is (224, 224, 3)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m resnet_output \u001b[38;5;241m=\u001b[39m TimeDistributed(cnn_model)(combined_input)\n\u001b[1;32m---> 27\u001b[0m resnet_output \u001b[38;5;241m=\u001b[39m \u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust pooling layer based on your requirements\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# resnet_output = Reshape((60, -1))(resnet_output)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(resnet_output.shape)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m bilstm_output \u001b[38;5;241m=\u001b[39m bilstm_model(resnet_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    119\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mconvert_shapes(input_shape, to_tuples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m input_dims \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(\n\u001b[0;32m    121\u001b[0m     tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mndims, input_shape)\n\u001b[0;32m    122\u001b[0m )\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m input_dims):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`TimeDistributed` Layer should be passed an `input_shape ` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith at least 3 dimensions, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Don't enforce the batch or time dimension.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "actions = len(subset_video_count.keys())\n",
    "\n",
    "# Define the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "# Additional layers\n",
    "bilstm_model.add(layers.Flatten())  # new added line -> (Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor.)\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(32, activation='relu'))\n",
    "bilstm_model.add(Dense(actions, activation = 'softmax'))\n",
    "\n",
    "# Combine the ResNet50 and BiLSTM models\n",
    "combined_input = Input(shape=(60, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
    "cnn_output = TimeDistributed(cnn_model)(combined_input)\n",
    "cnn_output = TimeDistributed(GlobalAveragePooling2D())(cnn_model)  # Adjust pooling layer based on your requirements\n",
    "# resnet_output = Reshape((60, -1))(resnet_output)\n",
    "# print(resnet_output.shape)\n",
    "bilstm_output = bilstm_model(cnn_output)\n",
    "\n",
    "# ---------------\n",
    "\n",
    "\n",
    "# Concatenate the outputs\n",
    "# merged = concatenate([resnet_output, bilstm_output])\n",
    "\n",
    "# Additional layers for combined model\n",
    "# final_output = Dense(128, activation='relu')(merged)\n",
    "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
    "\n",
    "# Create the final model\n",
    "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
    "\n",
    "# ----------------\n",
    "hybrid_model2 = Model(inputs=combined_input, outputs=bilstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9465718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(shape=(224, 224, 3)):\n",
    "    cnn_model = models.Sequential()\n",
    "    cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "    cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "    cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "    cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    output = GlobalMaxPool2D()\n",
    "    return Sequential([cnn_model, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d49b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_BiLSTM(shape=(60, 224, 224, 3), noActions=len(subset_video_count.keys())):    \n",
    "    cnn_model = build_cnn(shape[1:])\n",
    "    \n",
    "    # Define the BiLSTM model\n",
    "    bilstm_model = Sequential()\n",
    "    bilstm_model.add(TimeDistributed(cnn_model, input_shape=shape))\n",
    "    # bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "    bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "    bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "    # bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "    # Additional layers\n",
    "    bilstm_model.add(Dense(64, activation='relu'))\n",
    "    bilstm_model.add(Dropout(.5))\n",
    "    bilstm_model.add(Dense(32, activation='relu'))\n",
    "    bilstm_model.add(Dropout(.5))\n",
    "    bilstm_model.add(Dense(32, activation='relu'))\n",
    "    bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
    "\n",
    "    return bilstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3c1c20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model2 = CNN_BiLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92cf88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "hybrid_model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "952cc5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 74s 2s/step - loss: 0.7108 - accuracy: 0.5735 - val_loss: 0.7080 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history2  = hybrid_model2.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c27fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb46aa3d",
   "metadata": {},
   "source": [
    "Xception + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0139f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
    "Xnet_model=tf.keras.applications.xception.Xception(weights='imagenet',include_top=False, input_shape = (224, 224, 3))\n",
    "Xnet_model.trainable = False\n",
    "\n",
    "actions = len(subset_video_count.keys())\n",
    "\n",
    "# Define the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "# Additional layers\n",
    "# bilstm_model.add(layers.Flatten())  # new added line -> (Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor.)\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(32, activation='relu'))\n",
    "bilstm_model.add(Dense(actions, activation = 'softmax'))\n",
    "\n",
    "# Combine the ResNet50 and BiLSTM models\n",
    "combined_input = Input(shape=(60, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
    "Xnet_output = TimeDistributed(Xnet_model)(combined_input)\n",
    "Xnet_output = TimeDistributed(GlobalAveragePooling2D())(Xnet_model)  # Adjust pooling layer based on your requirements\n",
    "# resnet_output = Reshape((60, -1))(resnet_output)\n",
    "# print(resnet_output.shape)\n",
    "bilstm_output = bilstm_model(Xnet_output)\n",
    "\n",
    "# ---------------\n",
    "\n",
    "\n",
    "# Concatenate the outputs\n",
    "# merged = concatenate([resnet_output, bilstm_output])\n",
    "\n",
    "# Additional layers for combined model\n",
    "# final_output = Dense(128, activation='relu')(merged)\n",
    "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
    "\n",
    "# Create the final model\n",
    "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
    "\n",
    "# ----------------\n",
    "hybrid_model3 = Model(inputs=combined_input, outputs=bilstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3796679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "hybrid_model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd9175aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 60, 224, 224, 3   0         \n",
      "                             )]                                  \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 60, 7, 7, 2048)    23587712  \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 60, 2048)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 3)                 1129411   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24717123 (94.29 MB)\n",
      "Trainable params: 1129411 (4.31 MB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model summary\n",
    "hybrid_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62c69ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# frames = 60\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b1b702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 2981.3291 - accuracy: 0.3625WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "40/40 [==============================] - 264s 6s/step - loss: 2981.3291 - accuracy: 0.3625 - val_loss: 3343.4873 - val_accuracy: 0.4444\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 10839.7988 - accuracy: 0.2375WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "40/40 [==============================] - 246s 6s/step - loss: 10839.7988 - accuracy: 0.2375 - val_loss: 35102.9688 - val_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 38513.8633 - accuracy: 0.3125WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "40/40 [==============================] - 246s 6s/step - loss: 38513.8633 - accuracy: 0.3125 - val_loss: 29871.4375 - val_accuracy: 0.3333\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 19742.9902 - accuracy: 0.3625WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "40/40 [==============================] - 245s 6s/step - loss: 19742.9902 - accuracy: 0.3625 - val_loss: 101186.1094 - val_accuracy: 0.1111\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - ETA: 0s - loss: 43741.2734 - accuracy: 0.3875WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "40/40 [==============================] - 249s 6s/step - loss: 43741.2734 - accuracy: 0.3875 - val_loss: 18177.5293 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c38e59f6d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "history1  = hybrid_model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history1 = model.fit(X_train, y_train, epochs=5, \n",
    "#                     validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a322f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_model1.save('RES50BiModel_sub.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "171cdc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dr2007\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# hybrid_model1.save(\"RES50BiModel_sub.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b31773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834621b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce2bbe96",
   "metadata": {},
   "source": [
    "## Monitoring the Model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec95797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43f9c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(history1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa10d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18907.134766</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>4669.265137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss  accuracy     val_loss  val_accuracy\n",
       "0  18907.134766  0.441176  4669.265137           0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d920073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparing the training and validation loss.\n",
    "metrics_df[[\"loss\",\"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e9e158b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['binary_accuracy', 'val_binary_accuracy'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# comparing the training and validation accuracy.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmetrics_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_binary_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['binary_accuracy', 'val_binary_accuracy'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# comparing the training and validation accuracy.\n",
    "metrics_df[[\"binary_accuracy\",\"val_binary_accuracy\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fa78f",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24fe185e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy and loss over time \n",
    "\n",
    "# Training history \n",
    "history_dict = history1.history \n",
    "\n",
    "# Seperating validation and training accuracy \n",
    "acc = history_dict['accuracy'] \n",
    "val_acc = history_dict['val_accuracy'] \n",
    "\n",
    "# Seperating validation and training loss \n",
    "loss = history_dict['loss'] \n",
    "val_loss = history_dict['val_loss'] \n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(acc, label='accuracy') \n",
    "plt.plot(val_acc, label = 'val_accuracy') \n",
    "plt.title('Training and Validation Accuracy') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend(['Accuracy', 'Validation Accuracy']) \n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "# plt.subplot(1, 2, 2) \n",
    "# plt.plot(loss) \n",
    "# plt.plot(val_loss) \n",
    "# plt.title('Training and Validation Loss') \n",
    "# plt.xlabel('Epochs') \n",
    "# plt.ylabel('Loss') \n",
    "# plt.legend(['Loss', 'Validation Loss']) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b275fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 24s 24s/step - loss: 17.1240 - accuracy: 0.5000\n",
      "Accuracy on validation dataset: 0.5\n",
      "Loss on validation dataset: 17.123992919921875\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = hybrid_model1.evaluate(X_val, y_val)\n",
    "print('Accuracy on validation dataset:', val_accuracy)\n",
    "print('Loss on validation dataset:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy on test dataset:', val_accuracy)\n",
    "print('Loss on test dataset:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "791ef0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_accuracy = hybrid_model2.evaluate(X_val, y_val)\n",
    "# print('Accuracy on validation dataset:', val_accuracy)\n",
    "# print('Loss on validation dataset:', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d38f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe17e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25186f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
