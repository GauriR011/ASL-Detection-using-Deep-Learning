{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7cf524",
   "metadata": {},
   "source": [
    "# PROJECT STARTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5a983",
   "metadata": {},
   "source": [
    "# Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fea4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# import mediapipe as mp\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89327d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-python\n",
    "# !pip3 install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ec015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3543af3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To save variable of the last session (avoid re-executing the cells)\n",
    "dill.dump_session('base_variables3.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load session variables\n",
    "# dill.load_session('base_variables1.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195f4da",
   "metadata": {},
   "source": [
    "# Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path of dataset\n",
    "# video_path = r\"C:\\Users\\revan\\Downloads\\00335.mp4\"\n",
    "# folder_path = r'D:\\WLASL Datasets\\Kaggle_WLASL_withVideosInClassFolders\\dataset\\SL'\n",
    "folder_path = r'D:\\FYP_HWU\\Videos'\n",
    "DATA_PATH = r'D:\\FYP_HWU'\n",
    "# output_csv = os.path.join('dummy_dataset2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of actions to train model with (11 classes or actions)\n",
    "# actions = np.array(['Hello', 'Thank You', 'I Love You', 'Namaste'])\n",
    "actions = [\n",
    "'accident', \n",
    "'call', \n",
    "'help', \n",
    "'man', \n",
    "'murder', \n",
    "'woman', \n",
    "'danger',\n",
    "'police', \n",
    "'follow',\n",
    "'child',\n",
    "'sick'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c87b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_actions = ['follow', 'child','police']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472179a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae7098",
   "metadata": {},
   "source": [
    "### Counting number of videos under each action in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "video_count = {}\n",
    "\n",
    "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for i in dirs: #loop through each of the keywords or actions in the dataset\n",
    "        if (i in actions): #if the keyword is present in the shortlisted list of actions\n",
    "            for root, dirs, files in os.walk(os.path.join(folder_path, i)):\n",
    "                video_count[i] = len(dirs) \n",
    "                break\n",
    "    break\n",
    "\n",
    "print(video_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95624701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_count = {'accident': 13, 'call': 12, 'child': 9, 'danger': 11, 'help': 14, 'man': 12, 'murder': 13, 'police': 10, 'sick': 10, 'woman': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_video_count = {'follow': 36, 'child': 36, 'police': 40}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cfdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_video_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f9e04",
   "metadata": {},
   "source": [
    "### Getting the video paths of the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8065a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a dictionary for the actions and the count of videos for the chosen actions to train the model\n",
    "# for action in actions:\n",
    "#     classes[action] = my_dict[action]\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = {}\n",
    "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    \n",
    "    for action in actions:\n",
    "        for root, dirs, files in os.walk(os.path.join(folder_path, action)):\n",
    "            for i in range (len(files)):\n",
    "                files[i] = os.path.join(folder_path, action, files[i])\n",
    "            video_paths[action] = files\n",
    "            break\n",
    "    break\n",
    "\n",
    "# print(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_video_path = {}\n",
    "# temp_video_path['follow'] = video_paths['follow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4317688",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(temp_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0144d09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(video_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1285b",
   "metadata": {},
   "source": [
    "### Creating folders to store frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating folder 'Frames' that will contain all the video frames\n",
    "# os.makedirs(os.makedirs(os.path.join(DATA_PATH,'Frames')))\n",
    "# os.makedirs(os.path.join(DATA_PATH,'Original_Frames'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d648538",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(DATA_PATH,'Frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'D:\\FYP_HWU'\n",
    "DATA_PATH = os.path.join(DATA_PATH,'Rotate_Frames')\n",
    "\n",
    "#Creating one folder for each action\n",
    "for action in video_paths.keys():\n",
    "    # 1 folder for each video of the action\n",
    "    for sequence in range(len(video_paths[action])):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b725def",
   "metadata": {},
   "source": [
    "### Extracting frames from videos and adding them to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c35c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #looping through each action\n",
    "# for action in video_paths.keys():\n",
    "    \n",
    "#     #Looping through each video of the action\n",
    "#     for sequence in range(len(video_paths[action])):\n",
    "        \n",
    "#         # Open the video file\n",
    "#         cap = cv2.VideoCapture(video_paths[action][sequence])\n",
    "        \n",
    "#         hc=[]\n",
    "        \n",
    "#         # Initializing variables\n",
    "#         frame_count = 0       # current frame count\n",
    "#         max_fc = 60          # maximum frame count (ranging from 65 to 70 frames)\n",
    "# #         DATA_PATH = os.path.join('Frames') #path to folder \"Frames\" to store the extracted frames\n",
    "        \n",
    "#         with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#             #while the video is accessible and the current frame count doesn't exceed the max frame count limit\n",
    "#             while (cap.isOpened() and frame_count < max_fc):\n",
    "#                 #reading the video frame\n",
    "#                 success, frame = cap.read()\n",
    "#                 #if there are frames\n",
    "#                 if success:\n",
    "#             #         print(\"SUCCESS\")\n",
    "#                     # Make detection\n",
    "#                     image, result = mediapipeHolistics(frame, holistic)\n",
    "\n",
    "#                     #Darwing landmarks on frames\n",
    "#                     draw_landmarks(image,result)\n",
    "\n",
    "#                     cv2.imshow(\"{} - {}\".format(action, sequence), image)\n",
    "                    \n",
    "#                     # saving the keypoints extracted (in numpy format)\n",
    "# #                     np.save(DATA_PATH, image)\n",
    "#                     keypoints = extract_keypoints(result)\n",
    "#                     npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "#                     np.save(npy_path, keypoints)\n",
    "                    \n",
    "#                     #saving the extracted frames (in jpg format)\n",
    "#                     cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "#                     frame_count+=1\n",
    "# #                     hc.append([join(DATA_PATH ,'/', action, '/', str(sequence), '/', 'frame'+ str(frame_count) + '.jpg'), action, frame_count])\n",
    "                    \n",
    "#                 else:\n",
    "#                     break\n",
    "                    \n",
    "#             # repeat last frame until we reach max frame count\n",
    "#             while frame_count < max_fc:\n",
    "#                 cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_count))\n",
    "#                 np.save(npy_path, keypoints)\n",
    "#                 frame_count+=1\n",
    "# #                     hc.append([join(DATA_PATH ,'/', action, '/', str(sequence), '/', 'frame'+ str(frame_count) + '.jpg'), action, frame_count])\n",
    "\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7761ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through each action\n",
    "for action in video_paths.keys():\n",
    "    \n",
    "    #Looping through each video of the action\n",
    "    for sequence in range(len(video_paths[action])):\n",
    "        \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_paths[action][sequence])\n",
    "        \n",
    "        # Initializing variables\n",
    "        frame_count = 0       # current frame count\n",
    "        max_fc = 60          # maximum frame count\n",
    "        \n",
    "\n",
    "        #while the video is accessible and the current frame count doesn't exceed the max frame count limit\n",
    "        while (cap.isOpened() and frame_count < max_fc):\n",
    "            #reading the video frame\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            #if there are frames\n",
    "            if success:\n",
    "                image = frame.copy()\n",
    "                cv2.imshow(\"{} - {}\".format(action, sequence), image)\n",
    "                \n",
    "                #saving the extracted frames (in jpg format)\n",
    "                cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "                frame_count+=1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # repeat last frame until we reach max frame count\n",
    "        while frame_count < max_fc:\n",
    "            cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
    "            frame_count+=1\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a303a8",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae365",
   "metadata": {},
   "source": [
    "By the end of this section, we will folders containing the images frames stored in jpg format for each video of each action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944047ec",
   "metadata": {},
   "source": [
    "## Data Augmentation \n",
    " Using:\n",
    " - Frame Mirroring \n",
    " - Changing contrast and brightness\n",
    " - Rotating \n",
    " \n",
    " Here, we take each video, extract frames and for each frame, apply the filters (which may wither include changing the contrast and brightness of the image frame, mirroring the image or extracting only the keypoints and the edges from the image), and store them as a separate video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11847f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackAndWhite(image): # takes image as a parameter\n",
    "    # Image color\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    new_image = enhancer.enhance(0)  \n",
    "    \n",
    "    # return np.array(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saturation(image): # takes image as a parameter\n",
    "    # Horizontally flipping the image\n",
    "    image = flipImage(image)\n",
    "    # Image color\n",
    "    enhancer = ImageEnhance.Color(image)\n",
    "    new_image = enhancer.enhance(1.5)  \n",
    "    \n",
    "    # return np.array(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipImage(image): # takes image as a parameter\n",
    "    # Converting Image to numpy array\n",
    "    new_image = np.array(image)\n",
    "    \n",
    "    # Horizontally flipping the image\n",
    "    image = cv2.flip(new_image, 1)\n",
    "    \n",
    "    # Converting numpy aray to image format\n",
    "    image = Image.fromarray(image.astype('uint8'))  \n",
    "    \n",
    "    # Returning the image in image format\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotateImage(image):\n",
    "    image  = image.rotate(-10)  #- or + -> left or right\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547ff254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the final number of videos under each Action \n",
    "def totalVideoCount(DATA_PATH):\n",
    "    finalVideoCount = {}\n",
    "    for root, dirs, files in os.walk(DATA_PATH):\n",
    "        for folder in dirs:\n",
    "            for root, dirs, files in os.walk(os.path.join(DATA_PATH, folder)):\n",
    "                finalVideoCount[folder] = len(dirs)\n",
    "                break\n",
    "    return finalVideoCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "videoCount = totalVideoCount(PATH)\n",
    "print(videoCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = os.path.join(DATA_PATH,'accident', '6', 'frame' + '11' +'.jpg')\n",
    "# print(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0549ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize = (224,224)\n",
    "# img = cv2.imread(IMAGE_PATH)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# # transposed  = img.rotate(-10) \n",
    "# # new_image = np.array(transposed)\n",
    "# new_image = crop_center_square(img)\n",
    "# new_image = cv2.resize(new_image, resize)\n",
    "# image = Image.fromarray(new_image.astype('uint8')) \n",
    "# new_image = rotateImage(image)\n",
    "# new_image.show()\n",
    "# # image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073adf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_count = dictionary that maps actions to the number of videos for that action\n",
    "# video_path = dictionary that maps actions to a list containing the path of videos under that action\n",
    "# max_fc = 60 (maximum number of frames extracted per video), variable already initialized earlier\n",
    "\n",
    "# loop through each video (sequence) for each action\n",
    "# create another sequece folder (my_dict[action] + count), where count is incremented with every sequence loop iteration\n",
    "# for loop for i in range(60):\n",
    "# get the image -> concat('frame', str(i))\n",
    "# apply filters\n",
    "# save the image in the new folder created\n",
    "\n",
    "max_fc = 60\n",
    "AUG_PATH = r'D:\\FYP_HWU\\Aug_Frames'\n",
    "PATH = r'D:\\FYP_HWU\\Rotate_Frames'\n",
    "DATA_PATH = r'D:\\FYP_HWU\\Frames'\n",
    "\n",
    "finalVideoCount = totalVideoCount(AUG_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in video_count.keys():\n",
    "    actionCount = finalVideoCount[action]\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(video_count[action]):\n",
    "        \n",
    "        #creating a folder to store the augmented images\n",
    "        folder_number = actionCount + sequence\n",
    "        if(folder_number < 50):\n",
    "            try:\n",
    "                os.makedirs(os.path.join(PATH, action, str(folder_number)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for frame_num in range (max_fc): #max frames = 60\n",
    "                IMAGE_PATH = os.path.join(DATA_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "                # Reading the image\n",
    "                image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "                #Converting numpy array to Image\n",
    "                image = Image.fromarray(image.astype('uint8')) \n",
    "\n",
    "                # Applying the filters\n",
    "#                 new_image = blackAndWhite(image)\n",
    "#                 new_image = flipImage(image)\n",
    "#                 new_image = saturation(image)\n",
    "                new_image = rotateImage(image)\n",
    "\n",
    "                # Converting Image to numpy array\n",
    "                new_image = np.array(new_image)\n",
    "\n",
    "                #Saving the image in the folder created\n",
    "                cv2.imwrite(os.path.join(PATH , str(action), str(folder_number), 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
    "\n",
    "                # Displaying Image\n",
    "                cv2.destroyAllWindows()\n",
    "        \n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f661404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bac943",
   "metadata": {},
   "source": [
    "## Resizing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a99f38",
   "metadata": {},
   "source": [
    "Resizing the images to 224 x 224 pixel size to feed to ResNet 50 pre trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e264f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping the center of the image (cropping out the extra background margins of the video.)\n",
    "def crop_center_square(frame): # takes image as a parameter\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize = (224,224)\n",
    "# IMAGE_PATH = r'C:\\Users\\revan\\Downloads\\frame3.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(IMAGE_PATH)\n",
    "# image = Image.fromarray(image.astype('uint8')) \n",
    "# # image.show()\n",
    "\n",
    "# new_image = np.array(image)\n",
    "# new_image = crop_center_square(new_image)\n",
    "# new_image = cv2.resize(new_image, resize)\n",
    "# image = Image.fromarray(new_image.astype('uint8')) \n",
    "# image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d53127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize the frames\n",
    "def resizeFrames(new_image, resize): # takes numpy array as parameter\n",
    "    new_image = crop_center_square(new_image)\n",
    "    new_image = cv2.resize(new_image, resize)\n",
    "    return new_image # returns a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fc = 60\n",
    "resize = (224,224)\n",
    "AUG_PATH = r'D:\\FYP_HWU\\Aug_Frames'\n",
    "FRAMES_PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "\n",
    "finalVideoCount = totalVideoCount(AUG_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in finalVideoCount.keys():\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(finalVideoCount[action]):\n",
    "\n",
    "        try:\n",
    "            os.makedirs(os.path.join(FRAMES_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for frame_num in range (max_fc): #max frames = 60\n",
    "            IMAGE_PATH = os.path.join(AUG_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "            # Reading the image\n",
    "            image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "            new_image = resizeFrames(image, resize)\n",
    "\n",
    "            #Saving the image in the folder created\n",
    "            cv2.imwrite(os.path.join(FRAMES_PATH , str(action), str(sequence), 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
    "\n",
    "            # Displaying Image\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f20f4",
   "metadata": {},
   "source": [
    "## Keypoint Extraction using MediaPipe Holistics\n",
    "Applying MediaPipe Keypoint Landmarks to the extracted frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b61ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #Holistic model (use for detections)\n",
    "mp_drawing = mp.solutions.drawing_utils #Drawing Utilities\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness = 1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4039ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capturing landmarks\n",
    "def mediapipeHolistics(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion -> BGR to RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) # Making Prediction \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion -> RGB to BGR\n",
    "    return image, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbe39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize landmarks, connecting the landmarks on the image (drawing only pose and had landmarks)\n",
    "def draw_landmarks(image, results):\n",
    "\n",
    "#     mp_drawing.draw_landmarks(image = image, \n",
    "#                               landmark_list = results.face_landmarks, \n",
    "#                               connections = mp_holistic.FACEMESH_CONTOURS,\n",
    "#                               landmark_drawing_spec = drawing_spec,\n",
    "#                               connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.pose_landmarks, \n",
    "                              connections = mp_holistic.POSE_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.left_hand_landmarks, \n",
    "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)\n",
    "    mp_drawing.draw_landmarks(image = image, \n",
    "                              landmark_list = results.right_hand_landmarks, \n",
    "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec = drawing_spec,\n",
    "                              connection_drawing_spec = drawing_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing hand, pose and face landmarks\n",
    "def extract_keypoints(result):\n",
    "    pose = np.array([[res.x,res.y,res.z,res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4)\n",
    "    leftHand = np.array([[res.x,res.y,res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    rightHand = np.array([[res.x,res.y,res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    face = np.array([[res.x,res.y,res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([pose, face, leftHand, rightHand])\n",
    "#     return np.concatenate([pose, leftHand, rightHand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = os.path.join(FRAMES_PATH,'accident', '8', 'frame' + '30' +'.jpg')\n",
    "# print(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25501cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resize = (224,224)\n",
    "# img = cv2.imread(IMAGE_PATH)\n",
    "# with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "#     # Making detection\n",
    "#     image, result = mediapipeHolistics(img, holistic)\n",
    "    \n",
    "#     #Darwing landmarks on frames\n",
    "#     draw_landmarks(image,result)\n",
    "\n",
    "# # #Saving the keypoints extracted\n",
    "# keypoints = extract_keypoints(result)    \n",
    "# print(len(keypoints))\n",
    "# image = Image.fromarray(image.astype('uint8')) \n",
    "# image.show()\n",
    "# # image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0907fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fc = 60\n",
    "RESIZE_PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
    "FRAMES_PATH = r'D:\\FYP_HWU\\MP_Frames'\n",
    "NP_PATH = r'D:\\FYP_HWU\\MP_npArray'\n",
    "\n",
    "finalVideoCount = totalVideoCount(RESIZE_PATH)\n",
    "\n",
    "#looping through each action\n",
    "for action in finalVideoCount.keys():\n",
    "    \n",
    "    #Looping through the count for videos for that action\n",
    "    for sequence in range(finalVideoCount[action]):\n",
    "        \n",
    "        # Creating folders to store frames and extracted landmarks\n",
    "        try:\n",
    "            os.makedirs(os.path.join(FRAMES_PATH, action, str(sequence)))\n",
    "            os.makedirs(os.path.join(NP_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "            for frame_num in range (max_fc): #max frames = 60\n",
    "                \n",
    "                # Getting the extracted frame from the folder\n",
    "                IMAGE_PATH = os.path.join(RESIZE_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
    "\n",
    "                # Reading the image\n",
    "                image = cv2.imread(IMAGE_PATH)\n",
    "\n",
    "                # Making detection\n",
    "                image, result = mediapipeHolistics(image, holistic)\n",
    "\n",
    "                #Darwing landmarks on frames\n",
    "                draw_landmarks(image,result)\n",
    "\n",
    "                #Saving the keypoints extracted\n",
    "                keypoints = extract_keypoints(result) \n",
    "                \n",
    "                #Storing keypoints in numpy format\n",
    "                npy_path = os.path.join(NP_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "#                 image = Image.fromarray(image.astype('uint8')) \n",
    "\n",
    "                #Saving the image in the folder created\n",
    "                cv2.imwrite(os.path.join(FRAMES_PATH , str(action), str(sequence), 'frame'+ str(frame_num) + '.jpg'), image)\n",
    "\n",
    "                # Displaying Image\n",
    "                cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21b136",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1b3b8",
   "metadata": {},
   "source": [
    "### Visualizing dataset frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74910e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# man (video 0, frame 22)\n",
    "img_man = image.load_img(\"{}/man/0/frame22.jpg\". format(DATA_PATH))\n",
    "print(\"ACTION: Man    Video: 0     Frame: 22\")\n",
    "img_man"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3d374",
   "metadata": {},
   "source": [
    "## Creating Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4a2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03592f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in 80:10:10 train:validation:test ratio\n",
    "def trainTestSplit(X,y):\n",
    "    x_train, x_temp, y_train, y_temp= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.5, random_state=42)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dc733",
   "metadata": {},
   "source": [
    "### Preparing Data with 'Mediapipe Landmarks' stored as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b274e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641b9556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'police': 40, 'follow': 36, 'child': 36}\n"
     ]
    }
   ],
   "source": [
    "# Creating subset to to test whether the model works fine\n",
    "total_video_count = totalVideoCount(DATA_PATH)\n",
    "temp_list = ['police', 'follow', 'child']\n",
    "subset_video_count = {}\n",
    "for temp in temp_list:\n",
    "    subset_video_count[temp] = total_video_count[temp]\n",
    "    \n",
    "print(subset_video_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88d74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting numeric labels for non numeric action category values\n",
    "# label_map = {label:num for num, label in enumerate(video_count.keys())} # all classes\n",
    "label_map = {label:num for num, label in enumerate(subset_video_count.keys())} #subset of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29263b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'police': 0, 'follow': 1, 'child': 2}\n"
     ]
    }
   ],
   "source": [
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33039bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_npArray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798440c4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUBSET CLASSES\n",
    "\n",
    "#features = non target columns    labels = target columns\n",
    "\n",
    "max_fc = 60\n",
    "features, labels = [], []\n",
    "# Iterating through each action\n",
    "for action in total_video_count.keys():\n",
    "    \n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(total_video_count[action]):\n",
    "        \n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        \n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            \n",
    "            # Declaring directory of the keypoints\n",
    "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            \n",
    "            # Loading the numpy array\n",
    "            res = np.load(IMAGE_PATH)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(res)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        features.append(frames)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL CLASSES\n",
    "\n",
    "max_fc = 60\n",
    "features, labels = [], []\n",
    "# Iterating through each action\n",
    "for action in subset_video_count.keys():\n",
    "    \n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(subset_video_count[action]):\n",
    "        \n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        \n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            \n",
    "            # Declaring directory of the keypoints\n",
    "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "            \n",
    "            # Loading the numpy array\n",
    "            res = np.load(IMAGE_PATH)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(res)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        features.append(frames)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(features).shape\n",
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62390ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(labels).shape\n",
    "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1] \n",
    "y = to_categorical(labels).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6512f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train, Validation and Test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163062cb",
   "metadata": {},
   "source": [
    "### Preparing Data with Extracted Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a52b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home Computer Paths\n",
    "IMAGE_PATH = r'C:\\Users\\revan\\Downloads\\frame101.jpg'\n",
    "DATA_PATH = r'C:\\Users\\revan\\Downloads\\BOOK.mp4'\n",
    "IM_PATH = r'C:\\Users\\revan\\Downloads\\frame101.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0557ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cap = cv2.VideoCapture(DATA_PATH)\n",
    "# # # DATAPATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
    "# frame = cv2.imread(IMAGE_PATH)\n",
    "# # frame = frame[:, :, [2, 1, 0]]\n",
    "# image = Image.fromarray(frame.astype('uint8')) \n",
    "# # new_image := np.array(image)\n",
    "# # cv2.imwrite(IM_PATH,new_image)\n",
    "# image.show()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the video count\n",
    "# video_count = {'accident': 40, 'call': 40, 'child': 36, 'danger': 40, 'follow': 36, 'help': 40, 'man': 40, 'murder': 40, 'police': 40, 'sick': 40, 'woman': 40}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea605fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# University Computer Paths\n",
    "\n",
    "# IMAGE_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Frames\\accident\\0\\frame0.jpg'\n",
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'\n",
    "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12974db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameEnhance(frame):\n",
    "    # Converting BGR -> RGB\n",
    "    frame = frame[:, :, [2, 1, 0]]         \n",
    "    # Normalize the pixel values\n",
    "    frame = frame / 255.0\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm as tqdm\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# values = range(3)\n",
    "# with tqdm(total=len(values)) as pbar:\n",
    "#     for i in values:\n",
    "#         pbar.write('processed: %d' %i)\n",
    "#         pbar.update(1)\n",
    "#         sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all classes\n",
    "max_fc = 60\n",
    "videoFrames=[]\n",
    "videolabels=[]\n",
    "# Iterating through each action\n",
    "for action in video_count.keys():\n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(20): # video_count[action]\n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        # Iterating through each landmark numpy array\n",
    "        for frame_num in range(max_fc):\n",
    "            # Loading the image frame\n",
    "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
    "            \n",
    "            # # Converting BGR -> RGB\n",
    "            # frame = frame[:, :, [2, 1, 0]]\n",
    "            \n",
    "            # # Normalize the pixel values\n",
    "            # frame = frame / 255.0\n",
    "\n",
    "            frame = frameEnhance(frame)\n",
    "            \n",
    "            # Appending to the list\n",
    "            frames.append(frame)\n",
    "            \n",
    "        # Adding to the dataset\n",
    "        videoFrames.append(frames)\n",
    "        videolabels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e369980",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Resized_Frames'\n",
    "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_Frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = np.array([253,244,243])\n",
    "# array = array/255.0\n",
    "# print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde04ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action police done\n",
      "Action follow done\n",
      "Action child done\n"
     ]
    }
   ],
   "source": [
    "# For subset of the classes (3 classes) [child, follow, police]\n",
    "max_fc = 60\n",
    "videoFrames=[]\n",
    "videolabels=[]\n",
    "# Iterating through each action\n",
    "for action in subset_video_count.keys():\n",
    "    # Iterting through each video in the action\n",
    "    for sequence in range(subset_video_count[action]):\n",
    "        # Declaring a list to store the frames of each video\n",
    "        frames = []\n",
    "        # Iterating through each frame\n",
    "        for frame_num in range(max_fc):\n",
    "            # Loading the image frame\n",
    "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
    "            \n",
    "            # # Converting BGR -> RGB\n",
    "            # frame = frame[:, :, [2, 1, 0]]\n",
    "            \n",
    "            # # Normalize the pixel values\n",
    "            # frame = frame / 255.0\n",
    "            frame = frameEnhance(frame)\n",
    "            \n",
    "#             videoFrames.append(frame)\n",
    "#             videolabels.append(label_map[action])\n",
    "            # Appending to the list\n",
    "            frames.append(frame)\n",
    "            cv2.destroyAllWindows()\n",
    "        # Adding to the dataset\n",
    "        videoFrames.append(frames)\n",
    "        videolabels.append(label_map[action])\n",
    "\n",
    "    print(\"Action {} done\".format(action)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e83168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(videoFrames).shape\n",
    "X = np.array(videoFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03dbe16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54813e46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(videolabels).shape\n",
    "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1] \n",
    "y = to_categorical(videolabels).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3062780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train, Validation and Test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eae675e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8de003a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5860b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 60, 224, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b2e1b",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b3c77",
   "metadata": {},
   "source": [
    "## Building model for MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0a4d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# image processing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# model / neural network\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d9e9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a763e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "# frames = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1caebf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283299f0",
   "metadata": {},
   "source": [
    "RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecf297a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs\\ckpt.model.h5\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(log_dir,\"ckpt.model.h5\")\n",
    "print(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e8484ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "# checkpoint = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        os.path.join(log_dir,\"ckpt.model.h5\"), save_best_only=True, monitor = \"val_acc\", mode = \"max\"\n",
    "    )\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70157764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee21ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59994c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b1a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d38d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the entire model\n",
    "# x = resnet_50.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(512, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(64, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# predictions = layers.Dense(5, activation='softmax')(x)\n",
    "# model = Model(inputs = resnet_50.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51bd75",
   "metadata": {},
   "source": [
    "BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7859fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  history=model.fit(x_train, y_train,\n",
    "#            batch_size=batch_size,\n",
    "#            epochs=20,\n",
    "#            validation_data=[x_test, y_test])\n",
    "#  print(history.history['loss'])\n",
    "#  print(history.history['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a57dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model \n",
    "# model = tf.keras.Sequential([ \n",
    "#     encoder, \n",
    "#     tf.keras.layers.Embedding(11, 64, mask_zero=True), \n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)), \n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), \n",
    "#     tf.keras.layers.Dense(64, activation='relu'), \n",
    "#     tf.keras.layers.Dense(1) \n",
    "# ]) \n",
    "  \n",
    "# # Compile the model \n",
    "# model.compile( \n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "#     optimizer=tf.keras.optimizers.Adam(), \n",
    "#     metrics=['accuracy'] \n",
    "# )\n",
    "\n",
    "\n",
    "# # Training the model and validating it on test set \n",
    "# history = model.fit( \n",
    "#     train_dataset,  \n",
    "#     epochs=5, \n",
    "#     validation_data=test_dataset, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d571e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu')))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_corssentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=20, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bilstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea75b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install --user numpy==1.22.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7e79b",
   "metadata": {},
   "source": [
    "# #-------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74368859",
   "metadata": {},
   "source": [
    "RESNET 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet50 model\n",
    "# resnet_50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "# for layer in resnet_50.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the entire model\n",
    "# x = resnet_50.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(512, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(256, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(128, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# x = layers.Dense(64, activation='relu')(x) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# predictions = layers.Dense(5, activation='softmax')(x)\n",
    "# model = Model(inputs = resnet_50.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0b438",
   "metadata": {},
   "source": [
    "3D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import h5py\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.initializers import Constant\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b657a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(layers.Conv3D(32,(3,3,3),activation='relu',input_shape=(16,16,16,1),bias_initializer=Constant(0.01)))\n",
    "# model.add(layers.Conv3D(32,(3,3,3),activation='relu',bias_initializer=Constant(0.01)))\n",
    "# model.add(layers.MaxPooling3D((2,2,2)))\n",
    "# model.add(layers.Conv3D(64,(3,3,3),activation='relu'))\n",
    "# model.add(layers.Conv3D(64,(2,2,2),activation='relu'))\n",
    "# model.add(layers.MaxPooling3D((2,2,2)))\n",
    "# model.add(layers.Dropout(0.6))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(256,'relu'))\n",
    "# model.add(layers.Dropout(0.7))\n",
    "# model.add(layers.Dense(128,'relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.Dense(10,'softmax'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5949e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(Adam(0.001),'categorical_crossentropy',['accuracy'])\n",
    "# model.fit(xtrain,ytrain,epochs=200,batch_size=32,verbose=1,validation_data=(xtest,ytest),callbacks=[EarlyStopping(patience=15)])\n",
    "# Testing the 3D-CNN\n",
    "# _, acc = model.evaluate(xtrain, ytrain)\n",
    "# print('training accuracy:', str(round(acc*100, 2))+'%')\n",
    "# _, acc = model.evaluate(xtest, ytest)\n",
    "# print('testing accuracy:', str(round(acc*100, 2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe6be3",
   "metadata": {},
   "source": [
    "TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e66322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# # Define the number of layers, heads, and dimensions of the model\n",
    "# num_layers = 6\n",
    "# num_heads = 8\n",
    "# d_model = 512\n",
    "# # Define the encoder and decoder layers\n",
    "# encoder_layers = [\n",
    "#     keras.layers.MultiHeadAttention(num_heads, d_model),\n",
    "#     keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Add(),\n",
    "#     keras.layers.LayerNormalization()\n",
    "# ]\n",
    "# decoder_layers = [\n",
    "#     keras.layers.MultiHeadAttention(num_heads, d_model),\n",
    "#     keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Add(),\n",
    "#     keras.layers.LayerNormalization()\n",
    "# ]\n",
    "# # Create the encoder and decoder\n",
    "# encoder = keras.layers.Encoder(encoder_layers, num_layers)\n",
    "# decoder = keras.layers.Decoder(decoder_layers, num_layers)\n",
    "# # Define the final model\n",
    "# model = keras.models.Transformer(encoder, decoder)\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = keras.optimizers.Adam()\n",
    "# loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "# # Train the model on your dataset\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f91f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e5d787",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0767fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, LSTM, Bidirectional, Input, concatenate, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6e189ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "actions = len(subset_video_count.keys())\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5eb139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 60, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet_model.trainable = False  # Freeze the weights of the ResNet50 model\n",
    "\n",
    "actions = len(subset_video_count.keys())\n",
    "\n",
    "# Define the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
    "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
    "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
    "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
    "\n",
    "# Additional layers\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(32, activation='relu'))\n",
    "bilstm_model.add(Dense(actions, activation = 'softmax'))\n",
    "\n",
    "# Combine the ResNet50 and BiLSTM models\n",
    "combined_input = Input(shape=(60, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
    "resnet_output = TimeDistributed(resnet_model)(combined_input)\n",
    "resnet_output = TimeDistributed(GlobalAveragePooling2D())(resnet_output)  # Adjust pooling layer based on your requirements\n",
    "# resnet_output = Reshape((60, -1))(resnet_output)\n",
    "print(resnet_output.shape)\n",
    "bilstm_output = bilstm_model(resnet_output)\n",
    "\n",
    "# ---------------\n",
    "\n",
    "\n",
    "# Concatenate the outputs\n",
    "# merged = concatenate([resnet_output, bilstm_output])\n",
    "\n",
    "# Additional layers for combined model\n",
    "# final_output = Dense(128, activation='relu')(merged)\n",
    "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
    "\n",
    "# Create the final model\n",
    "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
    "\n",
    "# ----------------\n",
    "hybrid_model = Model(inputs=combined_input, outputs=bilstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "hybrid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9175aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c69ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# frames = 60\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hybrid_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cdc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"resnet50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fa78f",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss over time \n",
    "\n",
    "# Training history \n",
    "history_dict = history.history \n",
    "\n",
    "# Seperating validation and training accuracy \n",
    "acc = history_dict['accuracy'] \n",
    "val_acc = history_dict['val_accuracy'] \n",
    "\n",
    "# Seperating validation and training loss \n",
    "loss = history_dict['loss'] \n",
    "val_loss = history_dict['val_loss'] \n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(acc) \n",
    "plt.plot(val_acc) \n",
    "plt.title('Training and Validation Accuracy') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend(['Accuracy', 'Validation Accuracy']) \n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(loss) \n",
    "plt.plot(val_loss) \n",
    "plt.title('Training and Validation Loss') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.legend(['Loss', 'Validation Loss']) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b275fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ef0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d38f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe17e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25186f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
