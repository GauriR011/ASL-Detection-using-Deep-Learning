{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e7cf524",
      "metadata": {
        "id": "0e7cf524"
      },
      "source": [
        "# PROJECT STARTS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f5a983",
      "metadata": {
        "id": "13f5a983"
      },
      "source": [
        "# Importing Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9fea4e4c",
      "metadata": {
        "id": "9fea4e4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dr2007\\AppData\\Local\\Temp\\ipykernel_11052\\2279094380.py:10: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "# import mediapipe as mp\n",
        "# import csv\n",
        "# import dill\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0e210218",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "# tf. __version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "08f9dd68",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# !pip3 install --upgrade pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qzZnzDEZ6EMD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzZnzDEZ6EMD",
        "outputId": "50b78854-bbd7-465e-a18f-20ba0499c96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "vFlxKLir5VJW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFlxKLir5VJW",
        "outputId": "983e6b9d-388f-4899-c984-31125d28782e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb 23 17:24:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 546.12                 Driver Version: 546.12       CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 2080      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| 25%   32C    P8              20W / 215W |    462MiB /  8192MiB |     14%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1740    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A      7552    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
            "|    0   N/A  N/A     10884    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
            "|    0   N/A  N/A     10948    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     11568    C+G   ...s\\Autodesk\\Autodesk AdSSO\\AdSSO.exe    N/A      |\n",
            "|    0   N/A  N/A     12932    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A     14632    C+G   ...up\\ui-launcher\\AdskAccessUIHost.exe    N/A      |\n",
            "|    0   N/A  N/A     16532    C+G   ...on\\HEX\\Creative Cloud UI Helper.exe    N/A      |\n",
            "|    0   N/A  N/A     16564    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     18124    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "89327d46",
      "metadata": {
        "id": "89327d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\dr2007\\.conda\\envs\\test_env_gpu\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dr2007\\.conda\\envs\\test_env_gpu\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
            "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 1.0/10.6 MB 33.0 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 3.7/10.6 MB 46.9 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.1/10.6 MB 56.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  10.6/10.6 MB 65.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 10.6/10.6 MB 59.5 MB/s eta 0:00:00\n",
            "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
            "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
            "Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.3.0\n"
          ]
        }
      ],
      "source": [
        "# !pip3 install opencv-python\n",
        "# !pip3 install dill\n",
        "# !pip3 install matplotlib\n",
        "# !pip3 install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3543af3",
      "metadata": {
        "id": "c3543af3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# To save variable of the last session (avoid re-executing the cells)\n",
        "# dill.dump_session('base_variables3.db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c5f7e7",
      "metadata": {
        "id": "54c5f7e7"
      },
      "outputs": [],
      "source": [
        "# To load session variables\n",
        "# dill.load_session('base_variables3.db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VZtO4i_r81hP",
      "metadata": {
        "id": "VZtO4i_r81hP"
      },
      "outputs": [],
      "source": [
        "# !unzip '/content/drive/MyDrive/NEW_MP_Frames.zip' > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e195f4da",
      "metadata": {
        "id": "e195f4da"
      },
      "source": [
        "# Frame Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7317d5c2",
      "metadata": {
        "id": "7317d5c2"
      },
      "outputs": [],
      "source": [
        "# Folder path of dataset\n",
        "folder_path = r'D:\\FYP_HWU\\Videos'\n",
        "DATA_PATH = r'D:\\FYP_HWU'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6b0e03",
      "metadata": {
        "id": "4b6b0e03"
      },
      "outputs": [],
      "source": [
        "# List of actions to train model with (11 classes or actions)\n",
        "actions = [\n",
        "'accident',\n",
        "'call',\n",
        "'help',\n",
        "'man',\n",
        "'murder',\n",
        "'woman',\n",
        "'danger',\n",
        "'police',\n",
        "'follow',\n",
        "'child',\n",
        "'sick'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c87b66",
      "metadata": {
        "id": "61c87b66"
      },
      "outputs": [],
      "source": [
        "temp_actions = ['follow', 'child','police']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472179a6",
      "metadata": {
        "id": "472179a6",
        "outputId": "734663c9-37c9-4ec3-d26a-73c6a33a3c14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(temp_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ae7098",
      "metadata": {
        "id": "b6ae7098"
      },
      "source": [
        "### Counting number of videos under each action in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fac5687",
      "metadata": {
        "id": "2fac5687"
      },
      "outputs": [],
      "source": [
        "# folder_path = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'\n",
        "folder_path = r'D:\\WLASL Datasets\\Kaggle_WLASL_withVideosInClassFolders\\dataset\\SL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256c79e4",
      "metadata": {
        "id": "256c79e4",
        "outputId": "c2991df3-07a9-4acd-9f4f-14663763c863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accident': 13, 'call': 12, 'child': 9, 'danger': 11, 'follow': 9, 'help': 14, 'man': 12, 'murder': 13, 'police': 10, 'sick': 10, 'woman': 11}\n"
          ]
        }
      ],
      "source": [
        "# Initializing variables\n",
        "video_count = {}\n",
        "folder_path = r'D:\\FYP_HWU\\NEW_Frames'\n",
        "\n",
        "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for i in dirs: #loop through each of the keywords or actions in the dataset\n",
        "        if (i in actions): #if the keyword is present in the shortlisted list of actions\n",
        "            for root, dirs, files in os.walk(os.path.join(folder_path, i)):\n",
        "                video_count[i] = len(dirs)\n",
        "                break\n",
        "    break\n",
        "\n",
        "print(video_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5e3aaa",
      "metadata": {
        "id": "fb5e3aaa"
      },
      "outputs": [],
      "source": [
        "# Initializing variables\n",
        "video_count = {}\n",
        "first_11_items = {}\n",
        "\n",
        "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for i in dirs: #loop through each of the keywords or actions in the dataset\n",
        "        for root, dirs, files in os.walk(os.path.join(folder_path, i)):\n",
        "            video_count[i] = len(files)\n",
        "            break\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0a8806",
      "metadata": {
        "id": "4e0a8806"
      },
      "outputs": [],
      "source": [
        "video_count1 = sorted(video_count.items(), key=lambda x:x[1], reverse = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a166d7",
      "metadata": {
        "id": "f5a166d7",
        "outputId": "f97100e7-a381-42d2-c7ef-ba480adf2584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cousin': 17, 'before': 16, 'cool': 16, 'thin': 16, 'drink': 15, 'go': 15, 'computer': 14, 'help': 14, 'inform': 14, 'take': 14, 'who': 14}\n"
          ]
        }
      ],
      "source": [
        "for idx, k in enumerate(video_count1):\n",
        "    if idx == 11: break\n",
        "    first_11_items[k[0]] = k[1]\n",
        "\n",
        "print(first_11_items)\n",
        "# print(video_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d4fdf7",
      "metadata": {
        "id": "d5d4fdf7"
      },
      "outputs": [],
      "source": [
        "# print(video_count1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95624701",
      "metadata": {
        "id": "95624701"
      },
      "outputs": [],
      "source": [
        "# video_count = {'accident': 13, 'call': 12, 'child': 9, 'danger': 11, 'help': 14, 'man': 12, 'murder': 13, 'police': 10, 'sick': 10, 'woman': 11}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2524b96f",
      "metadata": {
        "id": "2524b96f"
      },
      "outputs": [],
      "source": [
        "temp_video_count = {'follow': 36, 'child': 36, 'police': 40}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147cfdda",
      "metadata": {
        "id": "147cfdda",
        "outputId": "577189d6-e765-41cc-db73-dca79f3bfe50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(temp_video_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086f9e04",
      "metadata": {
        "id": "086f9e04"
      },
      "source": [
        "### Getting the video paths of the videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80710ca",
      "metadata": {
        "id": "f80710ca"
      },
      "outputs": [],
      "source": [
        "video_paths = {}\n",
        "\n",
        "# Creating a dictionary for all the actions/classes along with the count of videos for each action in the dataset\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "\n",
        "    for action in actions:\n",
        "        for root, dirs, files in os.walk(os.path.join(folder_path, action)):\n",
        "            for i in range (len(files)):\n",
        "                files[i] = os.path.join(folder_path, action, files[i])\n",
        "            video_paths[action] = files\n",
        "            break\n",
        "    break\n",
        "\n",
        "# print(video_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a6099e",
      "metadata": {
        "id": "64a6099e"
      },
      "outputs": [],
      "source": [
        "# converting the folderpaths to csv file\n",
        "df = pd.DataFrame.from_dict(video_paths, orient='index')\n",
        "df = df.transpose()\n",
        "df.to_csv('video_paths.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5827ab0e",
      "metadata": {
        "id": "5827ab0e"
      },
      "outputs": [],
      "source": [
        "# Reading a csv file to get the video paths\n",
        "def read_csv_to_dict(csv_file):\n",
        "    result_dict = {}\n",
        "\n",
        "    with open(csv_file, 'r') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "\n",
        "        # Read the header row to get the keys\n",
        "        keys = next(csv_reader, None)\n",
        "        if keys:\n",
        "            for key in keys:\n",
        "                result_dict[key] = []\n",
        "\n",
        "            # Read the rest of the rows and store values in the dictionary\n",
        "            for row in csv_reader:\n",
        "                for i in range(len(keys)):\n",
        "                    result_dict[keys[i]].append(row[i])\n",
        "\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7353fa",
      "metadata": {
        "id": "6b7353fa"
      },
      "outputs": [],
      "source": [
        "video_paths = read_csv_to_dict(\"video_paths.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4317688",
      "metadata": {
        "collapsed": true,
        "id": "e4317688"
      },
      "outputs": [],
      "source": [
        "print(temp_video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0144d09",
      "metadata": {
        "id": "c0144d09"
      },
      "outputs": [],
      "source": [
        "# print(video_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc1285b",
      "metadata": {
        "id": "0dc1285b"
      },
      "source": [
        "### Creating folders to store frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50bd1108",
      "metadata": {
        "id": "50bd1108"
      },
      "outputs": [],
      "source": [
        "#Creating folder 'Frames' that will contain all the video frames\n",
        "# os.makedirs(os.makedirs(os.path.join(DATA_PATH,'Frames')))\n",
        "# os.makedirs(os.path.join(DATA_PATH,'Original_Frames'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d648538",
      "metadata": {
        "collapsed": true,
        "id": "8d648538"
      },
      "outputs": [],
      "source": [
        "#Creating folder 'NEW_Frames' that will contain all the video frames\n",
        "DATA_PATH = r'D:\\FYP_HWU'\n",
        "os.makedirs(os.path.join(DATA_PATH,'NEW_Frames'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685c8ed0",
      "metadata": {
        "id": "685c8ed0"
      },
      "outputs": [],
      "source": [
        "#Creating one folder for each action\n",
        "for action in video_paths.keys():\n",
        "    # 1 folder for each video of the action\n",
        "    for sequence in range(len(video_paths[action])):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e300d2",
      "metadata": {
        "id": "b9e300d2"
      },
      "source": [
        "## Extracting frames from videos and adding them to folders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b725def",
      "metadata": {
        "id": "3b725def"
      },
      "source": [
        "### APPROACH 1\n",
        "Here, a fixed number of frames are extracted from each video (60 in the current case). If the extracted frames from a video are less than 60, then the last frame is repeated to reach the 60 count. If the video is too long, then then the first 60 frames of the video will be extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7761ec4",
      "metadata": {
        "id": "f7761ec4"
      },
      "outputs": [],
      "source": [
        "#looping through each action\n",
        "for action in video_paths.keys():\n",
        "\n",
        "    #Looping through each video of the action\n",
        "    for sequence in range(len(video_paths[action])):\n",
        "\n",
        "        # Open the video file\n",
        "        cap = cv2.VideoCapture(video_paths[action][sequence])\n",
        "\n",
        "        # Initializing variables\n",
        "        frame_count = 0       # current frame count\n",
        "        max_fc = 60          # maximum frame count\n",
        "\n",
        "\n",
        "        #while the video is accessible and the current frame count doesn't exceed the max frame count limit\n",
        "        while (cap.isOpened() and frame_count < max_fc):\n",
        "            #reading the video frame\n",
        "            success, frame = cap.read()\n",
        "\n",
        "            #if there are frames\n",
        "            if success:\n",
        "                image = frame.copy()\n",
        "                cv2.imshow(\"{} - {}\".format(action, sequence), image)\n",
        "\n",
        "                #saving the extracted frames (in jpg format)\n",
        "                cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
        "                frame_count+=1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # repeat last frame until we reach max frame count\n",
        "        while frame_count < max_fc:\n",
        "            cv2.imwrite(os.path.join(DATA_PATH , action, str(sequence), 'frame'+ str(frame_count) + '.jpg'), image)\n",
        "            frame_count+=1\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4526c39",
      "metadata": {
        "id": "d4526c39"
      },
      "source": [
        "### APPROACH 2\n",
        "Here too, a fixed number of frames are extracted from each video (20 in the current case). The frames will be extracted after dividing the complete video into equal parts and then choosing the frames from each part. This way we will be able to avoid the first few or last frames few of the video if no action is being depicted. Secondly, the basic movement of the action is captured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eec91fc",
      "metadata": {
        "id": "8eec91fc"
      },
      "outputs": [],
      "source": [
        "# how many frames to extract?\n",
        "count = []\n",
        "for action in video_paths.keys():\n",
        "        #Looping through each video of the action\n",
        "        for sequence in range(len(video_paths[action])):\n",
        "            cap = cv2.VideoCapture(video_paths[action][sequence])\n",
        "            total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "            count.append(total_frames)\n",
        "            cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80df5fac",
      "metadata": {
        "id": "80df5fac",
        "outputId": "3724ecc9-3fdc-4cff-b84d-dea6f826e992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.0\n"
          ]
        }
      ],
      "source": [
        "print(min(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3c8ad5",
      "metadata": {
        "id": "2b3c8ad5",
        "outputId": "29c9d7d2-0a94-4422-d199-4fa995f47937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "155.0\n"
          ]
        }
      ],
      "source": [
        "print(max(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb71138",
      "metadata": {
        "id": "ceb71138"
      },
      "outputs": [],
      "source": [
        "# tcount = []\n",
        "# cap = cv2.VideoCapture(video_paths['accident'][0])\n",
        "# total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "# tcount.append(total_frames)\n",
        "# cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dabb347c",
      "metadata": {
        "id": "dabb347c"
      },
      "outputs": [],
      "source": [
        "# print(tcount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d97258",
      "metadata": {
        "id": "21d97258"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = r'D:\\FYP_HWU\\NEW_Frames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d895aee",
      "metadata": {
        "id": "4d895aee"
      },
      "outputs": [],
      "source": [
        "def frameSkipping(video_paths, n):\n",
        "    for action in video_paths.keys():\n",
        "\n",
        "        #Looping through each video of the action\n",
        "        for sequence in range(len(video_paths[action])): #len(video_paths[action])\n",
        "            cap = cv2.VideoCapture(video_paths[action][sequence])\n",
        "\n",
        "            total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)  #getting the total frame count\n",
        "            frames_step = total_frames//n    # getting the number of frames to skip\n",
        "\n",
        "            for frame_num in range(n):\n",
        "                #here, we set the parameter 1 which is the frame number to the frame (i*frames_step)\n",
        "                cap.set(1,frame_num*frames_step)\n",
        "                success,image = cap.read()\n",
        "                #save your image\n",
        "                cv2.imwrite(os.path.join(DATA_PATH, action, str(sequence), 'frame'+ str(frame_num) + '.jpg'), image)\n",
        "\n",
        "            cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565ddac3",
      "metadata": {
        "id": "565ddac3"
      },
      "outputs": [],
      "source": [
        "frameSkipping(video_paths, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a303a8",
      "metadata": {
        "id": "61a303a8"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911ae365",
      "metadata": {
        "id": "911ae365"
      },
      "source": [
        "By the end of this section, we will folders containing the images frames stored in jpg format for each video of each action."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944047ec",
      "metadata": {
        "id": "944047ec"
      },
      "source": [
        "## Data Augmentation\n",
        " Using:\n",
        " - Frame Mirroring (Flipping the image horizontally)\n",
        " - Changing contrast, brightness and saturation levels\n",
        " - Rotating\n",
        "\n",
        " Here, we take each video, extract frames and for each frame, apply the filters (which may wither include changing the contrast and brightness of the image frame, mirroring the image or extracting only the keypoints and the edges from the image), and store them as a separate video.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11847f28",
      "metadata": {
        "id": "11847f28"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f5ca49",
      "metadata": {
        "id": "41f5ca49"
      },
      "outputs": [],
      "source": [
        "def blackAndWhite(image): # takes image as a parameter\n",
        "    # Image color\n",
        "    enhancer = ImageEnhance.Color(image)\n",
        "    new_image = enhancer.enhance(0)\n",
        "\n",
        "    # return np.array(new_image)\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26fa759c",
      "metadata": {
        "id": "26fa759c"
      },
      "outputs": [],
      "source": [
        "def saturation(image): # takes image as a parameter\n",
        "    # Horizontally flipping the image\n",
        "    image = flipImage(image)\n",
        "    # Image color\n",
        "    enhancer = ImageEnhance.Color(image)\n",
        "    new_image = enhancer.enhance(1.5)\n",
        "\n",
        "    # return np.array(new_image)\n",
        "    return new_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39d16e4",
      "metadata": {
        "id": "b39d16e4"
      },
      "outputs": [],
      "source": [
        "def flipImage(image): # takes image as a parameter\n",
        "    # Converting Image to numpy array\n",
        "    new_image = np.array(image)\n",
        "\n",
        "    # Horizontally flipping the image\n",
        "    image = cv2.flip(new_image, 1)\n",
        "\n",
        "    # Converting numpy aray to image format\n",
        "    image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "    # Returning the image in image format\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c715b708",
      "metadata": {
        "id": "c715b708"
      },
      "outputs": [],
      "source": [
        "def rotateImage(image):\n",
        "    image  = image.rotate(-8)  #- or + -> left or right\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110df0fe",
      "metadata": {
        "id": "110df0fe"
      },
      "outputs": [],
      "source": [
        "# NEW_PATH = r'D:\\FYP_HWU\\New folder'\n",
        "# resize = (224,224)\n",
        "\n",
        "# for frame_num in range (20):\n",
        "#     IMAGE_PATH = os.path.join(DATA_PATH, 'child', '7' , 'frame' + str(frame_num) + '.jpg')\n",
        "\n",
        "#     # Reading the image\n",
        "#     image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "#     # Resizing the Image\n",
        "#     new_image = resizeFrames(image, resize)\n",
        "\n",
        "#     #Converting numpy array to Image\n",
        "#     image = Image.fromarray(new_image.astype('uint8'))\n",
        "\n",
        "#     # Applying the filters\n",
        "#     new_image = rotateImage(image)\n",
        "\n",
        "#     # Converting Image to numpy array\n",
        "#     new_image = np.array(new_image)\n",
        "\n",
        "#     #Saving the image in the folder created\n",
        "#     cv2.imwrite(os.path.join(NEW_PATH , 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
        "\n",
        "#     # Displaying Image\n",
        "#     cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb7309f",
      "metadata": {
        "id": "0eb7309f"
      },
      "outputs": [],
      "source": [
        "# Cropping the center of the image (cropping out the extra background margins of the video.)\n",
        "def crop_center_square(frame): # takes image as a parameter\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8427b29c",
      "metadata": {
        "id": "8427b29c"
      },
      "outputs": [],
      "source": [
        "# Function to resize the frames\n",
        "def resizeFrames(new_image, resize): # takes numpy array as parameter\n",
        "    new_image = crop_center_square(new_image)\n",
        "    new_image = cv2.resize(new_image, resize)\n",
        "    return new_image # returns a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547ff254",
      "metadata": {
        "id": "547ff254"
      },
      "outputs": [],
      "source": [
        "# Getting the final number of videos under each Action\n",
        "def totalVideoCount(DATA_PATH):\n",
        "    finalVideoCount = {}\n",
        "    for root, dirs, files in os.walk(DATA_PATH):\n",
        "        for folder in dirs:\n",
        "            for root, dirs, files in os.walk(os.path.join(DATA_PATH, folder)):\n",
        "                finalVideoCount[folder] = len(dirs)\n",
        "                break\n",
        "    return finalVideoCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99d6836",
      "metadata": {
        "id": "d99d6836"
      },
      "outputs": [],
      "source": [
        "# PATH = r'D:\\FYP_HWU\\Resized_Frames'\n",
        "# videoCount = totalVideoCount(PATH)\n",
        "# print(videoCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed8d486",
      "metadata": {
        "id": "9ed8d486"
      },
      "outputs": [],
      "source": [
        "# IMAGE_PATH = os.path.join(DATA_PATH,'accident', '6', 'frame' + '11' +'.jpg')\n",
        "# print(IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0549ed",
      "metadata": {
        "id": "cf0549ed"
      },
      "outputs": [],
      "source": [
        "# resize = (224,224)\n",
        "# img = cv2.imread(IMAGE_PATH)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# # transposed  = img.rotate(-10)\n",
        "# # new_image = np.array(transposed)\n",
        "# new_image = crop_center_square(img)\n",
        "# new_image = cv2.resize(new_image, resize)\n",
        "# image = Image.fromarray(new_image.astype('uint8'))\n",
        "# new_image = rotateImage(image)\n",
        "# new_image.show()\n",
        "# # image.show()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca1c478",
      "metadata": {
        "id": "fca1c478",
        "outputId": "bd21a380-b287-4f2d-a841-1dfa0adaadcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accident': 13, 'call': 12, 'child': 9, 'danger': 11, 'follow': 9, 'help': 14, 'man': 12, 'murder': 13, 'police': 10, 'sick': 10, 'woman': 11}\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = r'D:\\FYP_HWU\\NEW_Frames'\n",
        "VideoCount = totalVideoCount(DATA_PATH)\n",
        "print(VideoCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073adf0b",
      "metadata": {
        "id": "073adf0b"
      },
      "outputs": [],
      "source": [
        "# max_fc = 20 (maximum number of frames extracted per video), variable already initialized earlier\n",
        "\n",
        "# loop through each video (sequence) for each action\n",
        "# create another sequece folder (my_dict[action] + count), where count is incremented with every sequence loop iteration\n",
        "# for loop for i in range(20):\n",
        "# get the image -> concat('frame', str(i))\n",
        "# apply filters\n",
        "# save the image in the new folder created\n",
        "\n",
        "max_fc = 20\n",
        "resize = (224,224)\n",
        "AUG_PATH = r'D:\\FYP_HWU\\NEW_AUG_Frames'\n",
        "DATA_PATH = r'D:\\FYP_HWU\\NEW_Frames'\n",
        "\n",
        "finalVideoCount = totalVideoCount(AUG_PATH)\n",
        "video_count = totalVideoCount(DATA_PATH)\n",
        "\n",
        "#looping through each action\n",
        "for action in video_count.keys():\n",
        "    actionCount = finalVideoCount[action]\n",
        "\n",
        "    #Looping through the count for videos for that action\n",
        "    for sequence in range(video_count[action]):\n",
        "\n",
        "        #creating a folder to store the augmented images\n",
        "        folder_number = actionCount + sequence\n",
        "        if(folder_number < 45):\n",
        "            try:\n",
        "                os.makedirs(os.path.join(AUG_PATH, action, str(folder_number)))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            for frame_num in range (max_fc):\n",
        "                IMAGE_PATH = os.path.join(AUG_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
        "\n",
        "                # Reading the image\n",
        "                image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "                # Resizing the Image\n",
        "#                 new_image = resizeFrames(image, resize)\n",
        "\n",
        "                #Converting numpy array to Image\n",
        "                image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "                # Applying the filters\n",
        "#                 new_image = blackAndWhite(image)\n",
        "#                 new_image = flipImage(image)\n",
        "#                 new_image = saturation(image)\n",
        "                new_image = rotateImage(image)\n",
        "\n",
        "                # Converting Image to numpy array\n",
        "                new_image = np.array(new_image)\n",
        "\n",
        "                #Saving the image in the folder created\n",
        "                cv2.imwrite(os.path.join(AUG_PATH , str(action), str(folder_number), 'frame'+ str(frame_num) + '.jpg'), new_image)\n",
        "\n",
        "                # Displaying Image\n",
        "                cv2.destroyAllWindows()\n",
        "\n",
        "        else:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f661404",
      "metadata": {
        "id": "0f661404",
        "outputId": "58108e35-d237-490e-8622-20f8cf0a6b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accident': 45, 'call': 45, 'child': 45, 'danger': 45, 'follow': 45, 'help': 45, 'man': 45, 'murder': 45, 'police': 45, 'sick': 45, 'woman': 45}\n"
          ]
        }
      ],
      "source": [
        "finalVideoCount = totalVideoCount(AUG_PATH)\n",
        "print(finalVideoCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bac943",
      "metadata": {
        "id": "26bac943"
      },
      "source": [
        "## Resizing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a99f38",
      "metadata": {
        "id": "86a99f38"
      },
      "source": [
        "Resizing the images to 224 x 224 pixel size to feed to ResNet 50 pre trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e264f5",
      "metadata": {
        "id": "e9e264f5"
      },
      "outputs": [],
      "source": [
        "# Cropping the center of the image (cropping out the extra background margins of the video.)\n",
        "def crop_center_square(frame): # takes image as a parameter\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d53127",
      "metadata": {
        "id": "44d53127"
      },
      "outputs": [],
      "source": [
        "# Function to resize the frames\n",
        "def resizeFrames(new_image, resize): # takes numpy array as parameter\n",
        "    new_image = crop_center_square(new_image)\n",
        "    new_image = cv2.resize(new_image, resize)\n",
        "    return new_image # returns a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d1cb57",
      "metadata": {
        "id": "b7d1cb57"
      },
      "outputs": [],
      "source": [
        "max_fc = 20\n",
        "resize = (224,224)\n",
        "AUG_PATH = r'D:\\FYP_HWU\\NEW_AUG_Frames'\n",
        "DATA_PATH = r'D:\\FYP_HWU\\NEW_Frames'\n",
        "\n",
        "videoCount = totalVideoCount(DATA_PATH) #getting a video count from original frame dataset\n",
        "\n",
        "#looping through each action\n",
        "for action in videoCount.keys():\n",
        "\n",
        "    #Looping through the count for videos for that action\n",
        "    for sequence in range(videoCount[action]):\n",
        "\n",
        "        #creating a folder in AUG frames folder\n",
        "        try:\n",
        "            os.makedirs(os.path.join(AUG_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # iterating through each frame in the original frame dataset\n",
        "        for frame_num in range (max_fc):\n",
        "            IMAGE_PATH = os.path.join(DATA_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
        "\n",
        "            # Reading the image\n",
        "            image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "            new_image = resizeFrames(image, resize)\n",
        "\n",
        "            #Saving the image in the folder created\n",
        "            cv2.imwrite(os.path.join(AUG_PATH, str(action), str(sequence), 'frame' + str(frame_num) + '.jpg'), new_image)\n",
        "\n",
        "            # Displaying Image\n",
        "            cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687f20f4",
      "metadata": {
        "id": "687f20f4"
      },
      "source": [
        "## Keypoint Extraction using MediaPipe Holistics\n",
        "Applying MediaPipe Keypoint Landmarks to the extracted frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b61ee5",
      "metadata": {
        "id": "b2b61ee5"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic #Holistic model (use for detections)\n",
        "mp_drawing = mp.solutions.drawing_utils #Drawing Utilities\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness = 1, circle_radius=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca65d26",
      "metadata": {
        "id": "8ca65d26"
      },
      "outputs": [],
      "source": [
        "# # max_fc = 20\n",
        "# AUG_PATH = r'D:\\FYP_HWU\\NEW_AUG_Frames\\accident\\0\\frame12.jpg'\n",
        "\n",
        "# # Reading the image\n",
        "# image = cv2.imread(AUG_PATH)\n",
        "\n",
        "# # Making detection\n",
        "# with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
        "#     image, result = mediapipeHolistics(image, holistic)\n",
        "\n",
        "# #Darwing landmarks on frames\n",
        "# draw_landmarks(image,result)\n",
        "\n",
        "# image = Image.fromarray(image.astype('uint8'))\n",
        "# image.show()\n",
        "# # Displaying Image\n",
        "# cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4039ce",
      "metadata": {
        "id": "8a4039ce"
      },
      "outputs": [],
      "source": [
        "#Capturing landmarks\n",
        "def mediapipeHolistics(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion -> BGR to RGB\n",
        "    image.flags.writeable = False\n",
        "    results = model.process(image) # Making Prediction\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion -> RGB to BGR\n",
        "    return image, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dbe39a",
      "metadata": {
        "id": "f7dbe39a"
      },
      "outputs": [],
      "source": [
        "#Visualize landmarks, connecting the landmarks on the image (drawing only pose and had landmarks)\n",
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image = image,\n",
        "                              landmark_list = results.pose_landmarks,\n",
        "                              connections = mp_holistic.POSE_CONNECTIONS,\n",
        "                              landmark_drawing_spec = drawing_spec,\n",
        "                              connection_drawing_spec = drawing_spec)\n",
        "    mp_drawing.draw_landmarks(image = image,\n",
        "                              landmark_list = results.left_hand_landmarks,\n",
        "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
        "                              landmark_drawing_spec = drawing_spec,\n",
        "                              connection_drawing_spec = drawing_spec)\n",
        "    mp_drawing.draw_landmarks(image = image,\n",
        "                              landmark_list = results.right_hand_landmarks,\n",
        "                              connections = mp_holistic.HAND_CONNECTIONS,\n",
        "                              landmark_drawing_spec = drawing_spec,\n",
        "                              connection_drawing_spec = drawing_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0907fea",
      "metadata": {
        "id": "d0907fea"
      },
      "outputs": [],
      "source": [
        "max_fc = 20\n",
        "AUG_PATH = r'D:\\FYP_HWU\\NEW_AUG_Frames'\n",
        "FRAMES_PATH = r'D:\\FYP_HWU\\NEW_MP_Frames'\n",
        "\n",
        "finalVideoCount = totalVideoCount(AUG_PATH)\n",
        "\n",
        "#looping through each action\n",
        "for action in finalVideoCount.keys():\n",
        "\n",
        "    #Looping through the count for videos for that action\n",
        "    for sequence in range(finalVideoCount[action]):\n",
        "\n",
        "        # Creating folders to store frames and extracted landmarks\n",
        "        try:\n",
        "            os.makedirs(os.path.join(FRAMES_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
        "            for frame_num in range (max_fc):\n",
        "\n",
        "                # Getting the extracted frame from the folder\n",
        "                IMAGE_PATH = os.path.join(AUG_PATH, str(action), str(sequence) , 'frame' + str(frame_num) + '.jpg')\n",
        "\n",
        "                # Reading the image\n",
        "                image = cv2.imread(IMAGE_PATH)\n",
        "\n",
        "                # Making detection\n",
        "                image, result = mediapipeHolistics(image, holistic)\n",
        "\n",
        "                #Darwing landmarks on frames\n",
        "                draw_landmarks(image,result)\n",
        "\n",
        "#                 image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "                #Saving the image in the folder created\n",
        "                cv2.imwrite(os.path.join(FRAMES_PATH , str(action), str(sequence), 'frame'+ str(frame_num) + '.jpg'), image)\n",
        "\n",
        "                # Displaying Image\n",
        "                cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd21b136",
      "metadata": {
        "id": "bd21b136"
      },
      "source": [
        "# Creating the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb1b3b8",
      "metadata": {
        "id": "7fb1b3b8"
      },
      "source": [
        "### Visualizing dataset frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "u7HxDaYT9iBd",
      "metadata": {
        "id": "u7HxDaYT9iBd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance\n",
        "from IPython.display import Image\n",
        "DATA_PATH = r'NEW_MP_Frames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74910e0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "74910e0f",
        "outputId": "4e2141b7-8473-4fc8-9ea9-07729d971b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACTION: Man    Video: 0     Frame: 22\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# man (video 0, frame 22)\n",
        "print(\"ACTION: Man    Video: 0     Frame: 22\")\n",
        "display(Image(filename=\"{}/man/0/frame15.jpg\". format(DATA_PATH)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f3d374",
      "metadata": {
        "id": "38f3d374"
      },
      "source": [
        "## Creating Labels and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ec1f3f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip3 install tensorflow --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "304ab091",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-win_amd64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
            "   ---------------------------------------- 0.0/14.7 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/14.7 MB 9.9 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.9/14.7 MB 11.9 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.0/14.7 MB 11.0 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.0/14.7 MB 11.0 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.0/14.7 MB 11.0 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 1.3/14.7 MB 5.0 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.1/14.7 MB 6.7 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 2.1/14.7 MB 6.7 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 2.1/14.7 MB 6.7 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 2.1/14.7 MB 6.7 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 3.0/14.7 MB 5.9 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 3.1/14.7 MB 6.3 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 3.1/14.7 MB 6.3 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 3.1/14.7 MB 6.3 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 3.4/14.7 MB 4.9 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 4.2/14.7 MB 5.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 4.2/14.7 MB 5.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 4.2/14.7 MB 5.9 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 4.2/14.7 MB 5.9 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.2/14.7 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.2/14.7 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.2/14.7 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.2/14.7 MB 5.8 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 6.9/14.7 MB 6.2 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 8.4/14.7 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 8.4/14.7 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 8.4/14.7 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 8.4/14.7 MB 7.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 9.4/14.7 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 9.4/14.7 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 9.4/14.7 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 9.4/14.7 MB 7.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 10.5/14.7 MB 7.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 10.5/14.7 MB 7.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 10.5/14.7 MB 7.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 10.5/14.7 MB 7.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 12.1/14.7 MB 7.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 14.7/14.7 MB 11.9 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script f2py.exe is installed in 'C:\\Users\\dr2007\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires clang~=5.0, which is not installed.\n",
            "tensorboard 2.6.0 requires google-auth<2,>=1.6.3, but you have google-auth 2.22.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires absl-py~=0.10, but you have absl-py 1.4.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires flatbuffers~=1.12, but you have flatbuffers 20210226132247 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# !pip3 uninstall numpy==1.20.1\n",
        "# !pip3 install numpy==1.23.5 --user\n",
        "# import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "413ea077",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.23.5'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np \n",
        "np.__version__  #'1.23.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a29f1304",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4d4a2a96",
      "metadata": {
        "id": "4d4a2a96"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "03592f2b",
      "metadata": {
        "id": "03592f2b"
      },
      "outputs": [],
      "source": [
        "# # Splitting data in 80:10:10 train:validation:test ratio\n",
        "# def trainTestSplit(X,y):\n",
        "#     x_train, x_temp, y_train, y_temp= train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#     x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.5, random_state=42)\n",
        "#     return x_train, y_train, x_val, y_val, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f12045ff",
      "metadata": {
        "id": "f12045ff"
      },
      "outputs": [],
      "source": [
        "def trainTestSplit(X,y):\n",
        "    x_train, x_val, y_train, y_val= train_test_split(X, y, test_size=0.04, random_state=42)\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b274e54d",
      "metadata": {
        "id": "b274e54d"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = r'NEW_MP_Frames'\n",
        "# DATA_PATH = r'D:\\FYP_HWU\\NEW_MP_Frames'\n",
        "DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_DATA\\NEW_MP_Frames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "FTXm-CUpEi36",
      "metadata": {
        "id": "FTXm-CUpEi36"
      },
      "outputs": [],
      "source": [
        "# Getting the final number of videos under each Action\n",
        "def totalVideoCount(DATA_PATH):\n",
        "    finalVideoCount = {}\n",
        "    for root, dirs, files in os.walk(DATA_PATH):\n",
        "        for folder in dirs:\n",
        "            for root, dirs, files in os.walk(os.path.join(DATA_PATH, folder)):\n",
        "                finalVideoCount[folder] = len(dirs)\n",
        "                break\n",
        "    return finalVideoCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "641b9556",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "641b9556",
        "outputId": "402ed957-9c2f-4d47-9f6c-6b97bbcb7277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'police': 45, 'follow': 45, 'child': 45, 'woman': 45}\n"
          ]
        }
      ],
      "source": [
        "# Creating subset to to test whether the model works fine\n",
        "total_video_count = totalVideoCount(DATA_PATH)\n",
        "temp_list = ['police', 'follow', 'child', 'woman']\n",
        "subset_video_count = {}\n",
        "for temp in temp_list:\n",
        "    subset_video_count[temp] = total_video_count[temp]\n",
        "\n",
        "print(subset_video_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "839e982b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accident': 45, 'call': 45, 'child': 45, 'danger': 45, 'follow': 45, 'help': 45, 'man': 45, 'murder': 45, 'police': 45, 'sick': 45, 'woman': 45}\n"
          ]
        }
      ],
      "source": [
        "# total_video_count = totalVideoCount(DATA_PATH)\n",
        "print(total_video_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "47d0ffa6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'police': 45, 'follow': 45, 'child': 45, 'woman': 45}\n"
          ]
        }
      ],
      "source": [
        "print(subset_video_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3609ab",
      "metadata": {
        "id": "4f3609ab"
      },
      "outputs": [],
      "source": [
        "# subset_video_count = {'police': 40, 'follow': 36}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f88d74c3",
      "metadata": {
        "id": "f88d74c3"
      },
      "outputs": [],
      "source": [
        "# Setting numeric labels for non numeric action category values\n",
        "# label_map = {label:num for num, label in enumerate(total_video_count.keys())} # all classes\n",
        "label_map = {label:num for num, label in enumerate(subset_video_count.keys())} #subset of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "29263b59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29263b59",
        "outputId": "fc7ada33-4fd6-4557-f654-9f2191c61b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'police': 0, 'follow': 1, 'child': 2, 'woman': 3}\n"
          ]
        }
      ],
      "source": [
        "print(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aK5lyRKAB6c",
      "metadata": {
        "id": "7aK5lyRKAB6c"
      },
      "source": [
        "### Preparing Data with 'Mediapipe Landmarks' stored as numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798440c4",
      "metadata": {
        "collapsed": true,
        "id": "798440c4"
      },
      "outputs": [],
      "source": [
        "# SUBSET CLASSES\n",
        "\n",
        "#features = non target columns    labels = target columns\n",
        "\n",
        "max_fc = 20\n",
        "features, labels = [], []\n",
        "\n",
        "# Iterating through each action\n",
        "for action in subset_video_count.keys():\n",
        "\n",
        "    # Iterting through each video in the action\n",
        "    for sequence in range(subset_video_count[action]):\n",
        "\n",
        "        # Declaring a list to store the frames of each video\n",
        "        frames = []\n",
        "\n",
        "        # Iterating through each landmark numpy array\n",
        "        for frame_num in range(max_fc):\n",
        "\n",
        "            # Declaring directory of the keypoints\n",
        "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
        "\n",
        "            # Loading the numpy array\n",
        "            res = np.load(IMAGE_PATH)\n",
        "\n",
        "            # Appending to the list\n",
        "            frames.append(res)\n",
        "\n",
        "        # Adding to the dataset\n",
        "        features.append(frames)\n",
        "        labels.append(label_map[action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b033ce87",
      "metadata": {
        "id": "b033ce87"
      },
      "outputs": [],
      "source": [
        "# ALL CLASSES\n",
        "\n",
        "max_fc = 20\n",
        "features, labels = [], []\n",
        "# Iterating through each action\n",
        "for action in subset_video_count.keys():\n",
        "\n",
        "    # Iterting through each video in the action\n",
        "    for sequence in range(subset_video_count[action]):\n",
        "\n",
        "        # Declaring a list to store the frames of each video\n",
        "        frames = []\n",
        "\n",
        "        # Iterating through each landmark numpy array\n",
        "        for frame_num in range(max_fc):\n",
        "\n",
        "            # Declaring directory of the keypoints\n",
        "            IMAGE_PATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
        "\n",
        "            # Loading the numpy array\n",
        "            res = np.load(IMAGE_PATH)\n",
        "\n",
        "            # Appending to the list\n",
        "            frames.append(res)\n",
        "\n",
        "        # Adding to the dataset\n",
        "        features.append(frames)\n",
        "        labels.append(label_map[action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cc4f53",
      "metadata": {
        "id": "44cc4f53"
      },
      "outputs": [],
      "source": [
        "# np.array(features).shape\n",
        "X = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62390ca3",
      "metadata": {
        "id": "62390ca3"
      },
      "outputs": [],
      "source": [
        "# np.array(labels).shape\n",
        "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1]\n",
        "y = to_categorical(labels).astype(int)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6512f0c",
      "metadata": {
        "id": "a6512f0c"
      },
      "outputs": [],
      "source": [
        "# Splitting into Train, Validation and Test\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163062cb",
      "metadata": {
        "id": "163062cb"
      },
      "source": [
        "### Preparing Data with Extracted Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a52b3c",
      "metadata": {
        "id": "c1a52b3c"
      },
      "outputs": [],
      "source": [
        "# # Home Computer Paths\n",
        "# IMAGE_PATH = r'C:\\Users\\revan\\Downloads\\frame101.jpg'\n",
        "# DATA_PATH = r'C:\\Users\\revan\\Downloads\\BOOK.mp4'\n",
        "# IM_PATH = r'C:\\Users\\revan\\Downloads\\frame101.npy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0557ab0",
      "metadata": {
        "id": "b0557ab0"
      },
      "outputs": [],
      "source": [
        "# # cap = cv2.VideoCapture(DATA_PATH)\n",
        "# # # DATAPATH = os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))\n",
        "# frame = cv2.imread(IMAGE_PATH)\n",
        "# # frame = frame[:, :, [2, 1, 0]]\n",
        "# image = Image.fromarray(frame.astype('uint8'))\n",
        "# # new_image := np.array(image)\n",
        "# # cv2.imwrite(IM_PATH,new_image)\n",
        "# image.show()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b580ea50",
      "metadata": {
        "id": "b580ea50"
      },
      "outputs": [],
      "source": [
        "# get the video count\n",
        "# video_count = {'accident': 40, 'call': 40, 'child': 36, 'danger': 40, 'follow': 36, 'help': 40, 'man': 40, 'murder': 40, 'police': 40, 'sick': 40, 'woman': 40}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea605fa",
      "metadata": {
        "id": "dea605fa"
      },
      "outputs": [],
      "source": [
        "# University Computer Paths\n",
        "\n",
        "# IMAGE_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Frames\\accident\\0\\frame0.jpg'\n",
        "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Aug_Frames'\n",
        "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\MP_Frames'\n",
        "# DATA_PATH = r'NEW_MP_Frames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "12974db6",
      "metadata": {
        "id": "12974db6"
      },
      "outputs": [],
      "source": [
        "def frameEnhance(frame):\n",
        "    # Converting BGR -> RGB\n",
        "    frame = frame[:, :, [2, 1, 0]]\n",
        "    # Normalize the pixel values\n",
        "    frame = frame / 255.0\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3900ef4b",
      "metadata": {
        "id": "3900ef4b"
      },
      "outputs": [],
      "source": [
        "# For ALL the classes\n",
        "max_fc = 20\n",
        "videoFrames=[]\n",
        "videolabels=[]\n",
        "\n",
        "# Iterating through each action\n",
        "for action in total_video_count.keys():\n",
        "\n",
        "    # Iterting through each video in the action\n",
        "    for sequence in range(total_video_count[action]):\n",
        "\n",
        "        # Declaring a list to store the frames of each video\n",
        "        frames = []\n",
        "\n",
        "        # Iterating through each frame\n",
        "        for frame_num in range(max_fc):\n",
        "\n",
        "            # Loading the image frame\n",
        "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
        "            frame = frameEnhance(frame)\n",
        "\n",
        "            # Appending to the list\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Adding to the dataset\n",
        "        videoFrames.append(frames)\n",
        "        videolabels.append(label_map[action])\n",
        "\n",
        "    print(\"Action {} done\".format(action))\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e369980",
      "metadata": {
        "id": "3e369980"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = r'C:\\Users\\dr2007\\Documents\\FYP_HWU\\Resized_Frames'\n",
        "# DATA_PATH = r'D:\\FYP_HWU\\MP_Frames'\n",
        "# DATA_PATH = r'NEW_MP_Frames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4511c9a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4511c9a3",
        "outputId": "bc9a08d8-e76d-457d-bf45-a560aa61ffb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dr2007\\Documents\\FYP_DATA\\NEW_MP_Frames\n"
          ]
        }
      ],
      "source": [
        "# print(total_video_count)\n",
        "print(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bde04ed0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde04ed0",
        "outputId": "e38cf799-3295-4099-af82-33ff9b279c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action police done\n",
            "Action follow done\n",
            "Action child done\n",
            "Action woman done\n"
          ]
        }
      ],
      "source": [
        "# For subset of the classes (3 classes) [child, follow, police]\n",
        "max_fc = 20\n",
        "videoFrames=[]\n",
        "videolabels=[]\n",
        "\n",
        "# Iterating through each action\n",
        "for action in subset_video_count.keys():\n",
        "\n",
        "    # Iterting through each video in the action\n",
        "    for sequence in range(subset_video_count[action]):\n",
        "\n",
        "        # Declaring a list to store the frames of each video\n",
        "        frames = []\n",
        "\n",
        "        # Iterating through each frame\n",
        "        for frame_num in range(max_fc):\n",
        "\n",
        "            # Loading the image frame\n",
        "            frame = cv2.imread(os.path.join(DATA_PATH, action, str(sequence), \"frame{}\".format(frame_num) + \".jpg\"))\n",
        "            frame = frameEnhance(frame)\n",
        "\n",
        "            # Appending to the list\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Adding to the dataset\n",
        "        videoFrames.append(frames)\n",
        "        videolabels.append(label_map[action])\n",
        "\n",
        "    print(\"Action {} done\".format(action))\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0e83168f",
      "metadata": {
        "id": "0e83168f"
      },
      "outputs": [],
      "source": [
        "# np.array(videoFrames).shape\n",
        "X = np.array(videoFrames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "26505e20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26505e20",
        "outputId": "7540d968-4e53-4086-f14a-7a5717db6bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180\n"
          ]
        }
      ],
      "source": [
        "print(len(videoFrames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "03dbe16b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dbe16b",
        "outputId": "dc08f0de-dea0-4e7c-c42c-ca9809edafed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(180, 20, 224, 224, 3)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "54813e46",
      "metadata": {
        "id": "54813e46"
      },
      "outputs": [],
      "source": [
        "# np.array(videolabels).shape\n",
        "# The below code will convert the target class to this format: [1,0,0], [0,1,0], [0,0,1]\n",
        "y = to_categorical(videolabels).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "95a7730a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a7730a",
        "outputId": "0477ac00-8460-417b-c905-19004a43fb37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(180, 4)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hK2LJOZY_e1T",
      "metadata": {
        "id": "hK2LJOZY_e1T"
      },
      "outputs": [],
      "source": [
        "# np.save('X_data.npy', X)\n",
        "# np.save('y_data.npy', y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e3062780",
      "metadata": {
        "id": "e3062780"
      },
      "outputs": [],
      "source": [
        "# Splitting into Train, Validation and Test\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = trainTestSplit(X, y)\n",
        "X_train, y_train, X_val, y_val = trainTestSplit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7eae675e",
      "metadata": {
        "id": "7eae675e",
        "outputId": "4f454d01-3ad9-439a-d5a1-b7c5a84adcf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(178, 20, 224, 224, 3)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8de003a7",
      "metadata": {
        "id": "8de003a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 4)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "5109c5f8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(174, 4)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a5860b87",
      "metadata": {
        "id": "a5860b87",
        "outputId": "cabc2f52-8e9a-4345-e212-541d1393348b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 20, 224, 224, 3)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8b2e1b",
      "metadata": {
        "id": "1f8b2e1b"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5b3c77",
      "metadata": {
        "id": "ef5b3c77"
      },
      "source": [
        "## Building model for MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a4d4d9",
      "metadata": {
        "id": "c0a4d4d9"
      },
      "outputs": [],
      "source": [
        "# tensorflow\n",
        "# import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# model / neural network\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9e9e36",
      "metadata": {
        "id": "7d9e9e36"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a763e40f",
      "metadata": {
        "id": "a763e40f"
      },
      "outputs": [],
      "source": [
        "num_classes = 11\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "# frames = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caebf3f",
      "metadata": {
        "id": "1caebf3f"
      },
      "outputs": [],
      "source": [
        "log_dir = r'D:\\FYP_HWU\\Logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa044f5",
      "metadata": {
        "id": "3fa044f5"
      },
      "outputs": [],
      "source": [
        "os.makedirs(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283299f0",
      "metadata": {
        "id": "283299f0"
      },
      "source": [
        "RESNET 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf297a9",
      "metadata": {
        "id": "ecf297a9",
        "outputId": "5764abf6-7677-43f3-8036-a93228309e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs\\ckpt.RES50Bi_model.h5\n"
          ]
        }
      ],
      "source": [
        "# folder_path = os.path.join(log_dir,\"ckpt.RES50Bi_model.h5\")\n",
        "folder_path = os.path.join(log_dir,\"ckpt.CNNBi_model.keras\")\n",
        "print(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8484ba",
      "metadata": {
        "id": "9e8484ba"
      },
      "outputs": [],
      "source": [
        "# defining a function to save the weights of best model\n",
        "# checkpoint = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "folder_path = os.path.join(log_dir,\"ckpt.RES50Bi_model.keras\")\n",
        "print(folder_path)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "        folder_path, save_best_only=True, monitor = \"val_accuracy\", mode = \"max\"\n",
        "    )\n",
        "\n",
        "# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70157764",
      "metadata": {
        "id": "70157764"
      },
      "outputs": [],
      "source": [
        "model = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434d805c",
      "metadata": {
        "id": "434d805c"
      },
      "outputs": [],
      "source": [
        "#defining the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee21ee1",
      "metadata": {
        "id": "5ee21ee1"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bef577",
      "metadata": {
        "id": "78bef577"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=50, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59994c6e",
      "metadata": {
        "id": "59994c6e"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cf39a7",
      "metadata": {
        "id": "12cf39a7"
      },
      "outputs": [],
      "source": [
        "# model.save(\"resnet50.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4b1a2d",
      "metadata": {
        "id": "6d4b1a2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d38d74",
      "metadata": {
        "id": "b2d38d74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d5d4ce",
      "metadata": {
        "id": "70d5d4ce"
      },
      "outputs": [],
      "source": [
        "# # build the entire model\n",
        "# x = resnet_50.output\n",
        "# x = layers.GlobalAveragePooling2D()(x)\n",
        "# x = layers.Dense(512, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(256, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(128, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(64, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# predictions = layers.Dense(5, activation='softmax')(x)\n",
        "# model = Model(inputs = resnet_50.input, outputs = predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b51bd75",
      "metadata": {
        "id": "7b51bd75"
      },
      "source": [
        "BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae9dd3f",
      "metadata": {
        "id": "aae9dd3f"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "# model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
        "# model.add(Bidirectional(LSTM(64)))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7859fb9",
      "metadata": {
        "id": "e7859fb9"
      },
      "outputs": [],
      "source": [
        "#  history=model.fit(x_train, y_train,\n",
        "#            batch_size=batch_size,\n",
        "#            epochs=20,\n",
        "#            validation_data=[x_test, y_test])\n",
        "#  print(history.history['loss'])\n",
        "#  print(history.history['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a57dd0",
      "metadata": {
        "id": "25a57dd0"
      },
      "outputs": [],
      "source": [
        "# Creating the model\n",
        "# model = tf.keras.Sequential([\n",
        "#     encoder,\n",
        "#     tf.keras.layers.Embedding(11, 64, mask_zero=True),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "#     tf.keras.layers.Dense(64, activation='relu'),\n",
        "#     tf.keras.layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(\n",
        "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#     optimizer=tf.keras.optimizers.Adam(),\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "\n",
        "# # Training the model and validating it on test set\n",
        "# history = model.fit(\n",
        "#     train_dataset,\n",
        "#     epochs=5,\n",
        "#     validation_data=test_dataset,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d571e00",
      "metadata": {
        "id": "7d571e00"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(noClasses, batch_size, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu')))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134b496e",
      "metadata": {
        "id": "134b496e"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', metrics=['categorical_accuracy'], loss = 'categorical_corssentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c1ba72",
      "metadata": {
        "id": "16c1ba72"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=20, callbacks = [checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1919f7e0",
      "metadata": {
        "id": "1919f7e0"
      },
      "outputs": [],
      "source": [
        "model.save(\"bilstm.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ae594e",
      "metadata": {
        "id": "c6ae594e"
      },
      "outputs": [],
      "source": [
        "# Summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be38d94c",
      "metadata": {
        "id": "be38d94c"
      },
      "outputs": [],
      "source": [
        "# %pip install --user tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ea75b4",
      "metadata": {
        "collapsed": true,
        "id": "73ea75b4"
      },
      "outputs": [],
      "source": [
        "# !pip3 install --user numpy==1.22.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd7e79b",
      "metadata": {
        "id": "9cd7e79b"
      },
      "source": [
        "### #-------------------------------------------------------------#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74368859",
      "metadata": {
        "id": "74368859"
      },
      "source": [
        "RESNET 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c1d361",
      "metadata": {
        "id": "c3c1d361"
      },
      "outputs": [],
      "source": [
        "# # ResNet50 model\n",
        "# resnet_50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
        "# for layer in resnet_50.layers:\n",
        "#     layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1a59cf",
      "metadata": {
        "id": "6f1a59cf"
      },
      "outputs": [],
      "source": [
        "# # build the entire model\n",
        "# x = resnet_50.output\n",
        "# x = layers.GlobalAveragePooling2D()(x)\n",
        "# x = layers.Dense(512, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(256, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(128, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(64, activation='relu')(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# predictions = layers.Dense(5, activation='softmax')(x)\n",
        "# model = Model(inputs = resnet_50.input, outputs = predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d0b438",
      "metadata": {
        "id": "c5d0b438"
      },
      "source": [
        "3D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5949e7",
      "metadata": {
        "id": "7c5949e7"
      },
      "outputs": [],
      "source": [
        "# model.compile(Adam(0.001),'categorical_crossentropy',['accuracy'])\n",
        "# model.fit(xtrain,ytrain,epochs=200,batch_size=32,verbose=1,validation_data=(xtest,ytest),callbacks=[EarlyStopping(patience=15)])\n",
        "# Testing the 3D-CNN\n",
        "# _, acc = model.evaluate(xtrain, ytrain)\n",
        "# print('training accuracy:', str(round(acc*100, 2))+'%')\n",
        "# _, acc = model.evaluate(xtest, ytest)\n",
        "# print('testing accuracy:', str(round(acc*100, 2))+'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30fe6be3",
      "metadata": {
        "id": "30fe6be3"
      },
      "source": [
        "TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a7f7b3",
      "metadata": {
        "id": "15a7f7b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f91f7b",
      "metadata": {
        "id": "51f91f7b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "77e5d787",
      "metadata": {
        "id": "77e5d787"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "258f25c4",
      "metadata": {
        "id": "258f25c4"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4d55ffb8",
      "metadata": {
        "id": "4d55ffb8"
      },
      "outputs": [],
      "source": [
        "log_dir = r'C:\\Users\\dr2007\\Documents\\FYP_DATA\\Logs'\n",
        "# os.makedirs('Logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b7a3f046",
      "metadata": {
        "id": "b7a3f046"
      },
      "outputs": [],
      "source": [
        "# defining a function to save the weights of best model\n",
        "# log_dir = r'Logs'\n",
        "folder_path = os.path.join(log_dir,\"ckpt.Xception_sub1.keras\")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "        folder_path, save_best_only=True, monitor = \"val_accuracy\", mode = \"max\"\n",
        "    )\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0767fa42",
      "metadata": {
        "id": "0767fa42"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed, Dropout, LSTM, Bidirectional, Input, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, Concatenate, Flatten,Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "# from keras.optimizers import Adam, RMSprop, SVM\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6639431",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e03c793d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "with tf.device('/cpu:0'):\n",
        "   x = tf.convert_to_tensor(X_train, np.float32)\n",
        "   y = tf.convert_to_tensor(y_train, np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "07976b6a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf. __version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779a45a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip3 install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n_GlfvUOI1JW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_GlfvUOI1JW",
        "outputId": "12ab986a-40fe-41b8-8a27-959eaa0e635a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b6e189ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e189ca",
        "outputId": "99e92dbe-6bbc-4517-a26c-9b49a570b2f4"
      },
      "outputs": [],
      "source": [
        "# actions = len(total_video_count.keys())\n",
        "# print(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1d3988",
      "metadata": {},
      "source": [
        "### ResNet50 + BiLSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d957cb8",
      "metadata": {
        "id": "5d957cb8"
      },
      "source": [
        "#### RESNET 50 + BiLSTM (224 x 224 images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5eb139c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5eb139c",
        "outputId": "8163566d-c4b2-444a-a701-67fe039bacbd"
      },
      "outputs": [],
      "source": [
        "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet_model.trainable = False  # Freeze the weights of the ResNet50 model\n",
        "\n",
        "noActions = len(subset_video_count.keys())\n",
        "\n",
        "# Define the BiLSTM model\n",
        "bilstm_model = Sequential()\n",
        "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
        "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(20, 2048)))\n",
        "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
        "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
        "\n",
        "# Additional layers\n",
        "bilstm_model.add(Dense(64, activation='relu'))\n",
        "bilstm_model.add(Dense(32, activation='relu'))\n",
        "bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
        "\n",
        "# Combine the ResNet50 and BiLSTM models\n",
        "combined_input = Input(shape=(20, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
        "resnet_output = TimeDistributed(resnet_model)(combined_input)\n",
        "resnet_output = TimeDistributed(GlobalAveragePooling2D())(resnet_output)  # Adjust pooling layer based on your requirements\n",
        "# resnet_output = Reshape((60, -1))(resnet_output)\n",
        "# print(resnet_output.shape)\n",
        "bilstm_output = bilstm_model(resnet_output)\n",
        "\n",
        "# ---------------\n",
        "\n",
        "\n",
        "# Concatenate the outputs\n",
        "# merged = concatenate([resnet_output, bilstm_output])\n",
        "\n",
        "# Additional layers for combined model\n",
        "# final_output = Dense(128, activation='relu')(merged)\n",
        "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
        "\n",
        "# Create the final model\n",
        "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
        "\n",
        "# ----------------\n",
        "hybrid_model1 = Model(inputs=combined_input, outputs=bilstm_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c5255150",
      "metadata": {
        "id": "c5255150"
      },
      "outputs": [],
      "source": [
        "def build_renet50():\n",
        "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    for layer in resnet_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # for layer in resnet_model.layers[:170]:\n",
        "    #     layer.trainable = False\n",
        "\n",
        "#     output = GlobalMaxPool2D()\n",
        "    return resnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4bf724f8",
      "metadata": {
        "id": "4bf724f8"
      },
      "outputs": [],
      "source": [
        "def build_BiLSTM(noActions, shape = (20, 2048)):\n",
        "\n",
        "    # Define the BiLSTM model\n",
        "    bilstm_model = Sequential()\n",
        "    # bilstm_model.add(TimeDistributed(first_model)(inputShape)) #(20, 2048)\n",
        "    bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=shape))) #, input_shape=(60, 2048)\n",
        "    bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "\n",
        "    # Additional layers\n",
        "    # bilstm_model.add(Flatten())\n",
        "    # bilstm_model.add(Dense(64, activation='relu'))\n",
        "    # bilstm_model.add(Dropout(.5))\n",
        "    # bilstm_model.add(Dense(128, activation='relu'))\n",
        "    # bilstm_model.add(Dropout(.5))\n",
        "    # bilstm_model.add(Dense(64, activation='relu'))\n",
        "    # bilstm_model.add(Dropout(.5))\n",
        "    # bilstm_model.add(Dense(32, activation='relu'))\n",
        "    # bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
        "\n",
        "    bilstm_model.add(BatchNormalization())\n",
        "    bilstm_model.add(Dense(256, activation='relu'))\n",
        "    bilstm_model.add(BatchNormalization())\n",
        "    bilstm_model.add(Dense(128, activation='relu'))\n",
        "    bilstm_model.add(Dropout(0.5))\n",
        "    bilstm_model.add(BatchNormalization())\n",
        "    bilstm_model.add(Dense(64, activation='relu'))\n",
        "    bilstm_model.add(Dropout(0.5))\n",
        "    bilstm_model.add(BatchNormalization())\n",
        "    bilstm_model.add(Dense(noActions, activation='softmax'))\n",
        "\n",
        "    return bilstm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c015b470",
      "metadata": {
        "id": "c015b470"
      },
      "outputs": [],
      "source": [
        "def Res50_BiLSTM(inputShape=(20,224,224,3), noActions=len(subset_video_count.keys())):\n",
        "    combined_input = Input(shape=inputShape)  # Assuming image shape is (224, 224, 3)\n",
        "    resnetModel = build_renet50()\n",
        "    resnet_output = TimeDistributed(resnetModel)(combined_input)\n",
        "    resnet_output = TimeDistributed(GlobalAveragePooling2D())(resnet_output)\n",
        "    bilstm_model = build_BiLSTM(noActions, (20, 2048))\n",
        "    bilstm_output = bilstm_model(resnet_output)\n",
        "    hybrid_model1 = Model(inputs=combined_input, outputs=bilstm_output)\n",
        "    return hybrid_model1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d970510f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "log_dir = r'C:\\Users\\dr2007\\Documents\\FYP_DATA\\Logs'\n",
        "folder_path = os.path.join(log_dir,\"ckpt.Res50_sub1.keras\")\n",
        "res50LoadModel=load_model(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a3e83d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RESNET 50 CODE\n",
        "def build_renet50():\n",
        "    # resnet_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # # Freeze the ResNet50 layers\n",
        "    # for layer in resnet_model.layers:\n",
        "    #     layer.trainable = False\n",
        "\n",
        "    # Input layer for the video frames\n",
        "    input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "    # Apply TimeDistributed to the ResNet50 base\n",
        "    resnet_output = res50LoadModel(input_frames)\n",
        "    # outputs = res50LoadModel(input_frames)\n",
        "\n",
        "    return Model(input_frames, resnet_output, name=\"feature_extractor\")\n",
        "\n",
        "res50_fe = build_renet50()\n",
        "\n",
        "\n",
        "# def build_feature_extractor():\n",
        "#     feature_extractor = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "#     preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "#     preprocessed = preprocess_input(inputs)\n",
        "\n",
        "#     outputs = feature_extractor(preprocessed)\n",
        "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe136f6",
      "metadata": {},
      "source": [
        "#### ResNet50 Model (Alone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "873a8746",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet50 without the top (fully connected) layers\n",
        "resnet_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the ResNet50 layers\n",
        "for layer in resnet_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Input layer for the video frames\n",
        "input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "# Apply TimeDistributed to the ResNet50 base\n",
        "resnet_output = TimeDistributed(resnet_model)(input_frames)\n",
        "\n",
        "# Global Average Pooling layer to reduce spatial dimensions\n",
        "global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(resnet_output)\n",
        "\n",
        "# Fully connected layers for video-level classification\n",
        "dense_layer = (Flatten())(global_avg_pool)\n",
        "dense_layer = Dense(512, activation='relu')(dense_layer)\n",
        "output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "\n",
        "# Create the final model\n",
        "hybrid_model1 = tf.keras.models.Model(inputs=input_frames, outputs=output_layer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "ea2ce1a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model (you can customize the optimizer, loss, and metrics)\n",
        "hybrid_model1.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "d6f2bd42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "87/87 [==============================] - 8s 84ms/step - loss: 0.3661 - accuracy: 0.8736 - val_loss: 0.6594 - val_accuracy: 0.8333\n",
            "Epoch 2/20\n",
            "87/87 [==============================] - 7s 82ms/step - loss: 0.3858 - accuracy: 0.8678 - val_loss: 0.3778 - val_accuracy: 0.8333\n",
            "Epoch 3/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.3069 - accuracy: 0.8908 - val_loss: 0.6817 - val_accuracy: 0.8333\n",
            "Epoch 4/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.3417 - accuracy: 0.8506 - val_loss: 0.6554 - val_accuracy: 0.6667\n",
            "Epoch 5/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.2811 - accuracy: 0.9368 - val_loss: 0.4093 - val_accuracy: 0.8333\n",
            "Epoch 6/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.2359 - accuracy: 0.9368 - val_loss: 0.7822 - val_accuracy: 0.6667\n",
            "Epoch 7/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.2511 - accuracy: 0.9195 - val_loss: 0.2036 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "87/87 [==============================] - 7s 80ms/step - loss: 0.2962 - accuracy: 0.8736 - val_loss: 1.2148 - val_accuracy: 0.6667\n",
            "Epoch 9/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.2768 - accuracy: 0.9080 - val_loss: 0.2365 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "87/87 [==============================] - 7s 83ms/step - loss: 0.3501 - accuracy: 0.8506 - val_loss: 0.2135 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.1963 - accuracy: 0.9368 - val_loss: 0.4043 - val_accuracy: 0.8333\n",
            "Epoch 12/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.3133 - accuracy: 0.8621 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "87/87 [==============================] - 7s 83ms/step - loss: 0.2078 - accuracy: 0.9425 - val_loss: 0.6252 - val_accuracy: 0.6667\n",
            "Epoch 14/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.2170 - accuracy: 0.9253 - val_loss: 0.1490 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.1713 - accuracy: 0.9425 - val_loss: 0.9629 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.3544 - accuracy: 0.8333 - val_loss: 1.0432 - val_accuracy: 0.6667\n",
            "Epoch 17/20\n",
            "87/87 [==============================] - 7s 82ms/step - loss: 0.1840 - accuracy: 0.9425 - val_loss: 0.3576 - val_accuracy: 0.8333\n",
            "Epoch 18/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.1360 - accuracy: 0.9598 - val_loss: 0.8331 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.4285 - accuracy: 0.8506 - val_loss: 0.4606 - val_accuracy: 0.6667\n",
            "Epoch 20/20\n",
            "87/87 [==============================] - 7s 81ms/step - loss: 0.4037 - accuracy: 0.8448 - val_loss: 0.7124 - val_accuracy: 0.6667\n"
          ]
        }
      ],
      "source": [
        "history = hybrid_model1.fit(x, y, epochs=20, batch_size=batch_size, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f846598",
      "metadata": {},
      "source": [
        "#### BiLSTM Model (Alone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bbbffe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the BiLSTM model\n",
        "# bilstm_model = Sequential()\n",
        "# # bilstm_model.add(TimeDistributed(first_model)(inputShape)) #(20, 2048)\n",
        "\n",
        "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True))) #, input_shape=(60, 2048)\n",
        "# bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "\n",
        "# input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "# # Apply TimeDistributed to the ResNet50 base\n",
        "# resnet_output = TimeDistributed(bilstm_model)(input_frames)\n",
        "\n",
        "# # Global Average Pooling layer to reduce spatial dimensions\n",
        "# global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(resnet_output)\n",
        "\n",
        "# # Fully connected layers for video-level classification\n",
        "# dense_layer = (Flatten())(global_avg_pool)\n",
        "# dense_layer = Dense(512, activation='relu')(dense_layer)\n",
        "# output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "# ==========================================\n",
        "\n",
        "# # Reshape to fit the LSTM input shape\n",
        "# input_frames = Input(shape=(20, 224, 224, 3))\n",
        "# reshaped_frames = tf.keras.layers.Reshape((20, -1))(input_frames)\n",
        "# # print(reshaped_frames)\n",
        "# # Bidirectional LSTM layer\n",
        "# bilstm_output = TimeDistributed(Bidirectional(LSTM(256, return_sequences=True)))(reshaped_frames)\n",
        "# bilstm_output = TimeDistributed(Bidirectional(LSTM(64, return_sequences=False)))(bilstm_output)\n",
        "\n",
        "# # Global Average Pooling layer to reduce temporal dimensions\n",
        "# global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(bilstm_output)\n",
        "\n",
        "# # Fully connected layers for video-level classification\n",
        "# dense_layer = (Flatten())(global_avg_pool)\n",
        "# dense_layer = Dense(512, activation='relu')(dense_layer)\n",
        "# output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "\n",
        "# # Create the final model\n",
        "# hybrid_model2 = Model(inputs=input_frames, outputs=output_layer)\n",
        "\n",
        "#===========================================================\n",
        "\n",
        "# Input layer for the video frames\n",
        "input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "# Reshape to fit the LSTM input shape\n",
        "reshaped_frames = TimeDistributed(Flatten())(input_frames)\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "bilstm_output = Bidirectional(LSTM(64, return_sequences=True))(reshaped_frames)\n",
        "bilstm_output = Bidirectional(LSTM(32, return_sequences=False))(bilstm_output)\n",
        "\n",
        "# Global Average Pooling layer to reduce temporal dimensions\n",
        "global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(bilstm_output)\n",
        "\n",
        "# Fully connected layers for video-level classification\n",
        "dense_layer = (Flatten())(global_avg_pool)\n",
        "dense_layer = Dense(64, activation='relu')(dense_layer)\n",
        "output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "\n",
        "# Create the final model\n",
        "hybrid_model2 = Model(inputs=input_frames, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b9edcb3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "28455647",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a learning rate schedule function\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def lr_schedule(epoch, current_lr):\n",
        "    if epoch < 3:\n",
        "        return current_lr  # Keep the initial learning rate for the first 5 epochs\n",
        "    else:\n",
        "        # Increase the learning rate by 3 times after the 5th epoch\n",
        "        return current_lr * 0.1\n",
        "    \n",
        "\n",
        "# Create a learning rate scheduler\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7d241830",
      "metadata": {
        "id": "7d241830"
      },
      "outputs": [],
      "source": [
        "Res50_BiLSTM_1 = Res50_BiLSTM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30e38eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# #--------------------------------------------------\n",
        "# resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# resnet_model.trainable = False  # Freeze the weights of the ResNet50 model\n",
        "\n",
        "# noActions = len(subset_video_count.keys())\n",
        "\n",
        "# # Define the BiLSTM model\n",
        "# bilstm_model = Sequential()\n",
        "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu', input_shape=(20, 2048))))\n",
        "# bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation='relu')))\n",
        "\n",
        "# # Additional layers\n",
        "# bilstm_model.add(Flatten())\n",
        "# bilstm_model.add(Dense(128, activation='relu'))\n",
        "# bilstm_model.add(Dropout(.5))\n",
        "# bilstm_model.add(Dense(64, activation='relu'))\n",
        "# bilstm_model.add(Dropout(.5))\n",
        "# bilstm_model.add(Dense(32, activation='relu'))\n",
        "# bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
        "\n",
        "# combined_input = Input(shape=(20, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
        "# resnet_output = TimeDistributed(resnet_model())(combined_input)\n",
        "# resnet_output = TimeDistributed(GlobalAveragePooling2D())(resnet_output)\n",
        "# bilstm_output = bilstm_model(resnet_output)\n",
        "# Res50_BiLSTM_1 = Model(inputs=combined_input, outputs=bilstm_output)\n",
        "\n",
        "# #--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e74a436d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "# from keras.optimizers import Adam, RMSprop, SVM\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "a2b9a29d",
      "metadata": {
        "id": "a2b9a29d"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "# Res50_BiLSTM_1.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "Res50_BiLSTM_1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "75bfe002",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 20, 224, 224, 3)] 0         \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 7, 7, 2048)    23587712  \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 2048)          0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 4)                 1183172   \n",
            "=================================================================\n",
            "Total params: 24,770,884\n",
            "Trainable params: 1,182,148\n",
            "Non-trainable params: 23,588,736\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Res50_BiLSTM_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "113ebabe",
      "metadata": {
        "id": "113ebabe"
      },
      "outputs": [],
      "source": [
        "# batch_size = 2\n",
        "# epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0ec1fef5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# with tf.device('/cpu:0'):\n",
        "#    x = tf.convert_to_tensor(X_train, np.float32)\n",
        "#    y = tf.convert_to_tensor(y_train, np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "8FAcVzbGCWmU",
      "metadata": {
        "id": "8FAcVzbGCWmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "86/86 [==============================] - 21s 124ms/step - loss: 1.6086 - accuracy: 0.2456 - val_loss: 153.2892 - val_accuracy: 0.4444\n",
            "Epoch 2/7\n",
            "86/86 [==============================] - 8s 94ms/step - loss: 1.5351 - accuracy: 0.2573 - val_loss: 80.8989 - val_accuracy: 0.4444\n",
            "Epoch 3/7\n",
            "86/86 [==============================] - 8s 94ms/step - loss: 1.5545 - accuracy: 0.2515 - val_loss: 316.3369 - val_accuracy: 0.4444\n",
            "Epoch 4/7\n",
            "86/86 [==============================] - 8s 94ms/step - loss: 1.5110 - accuracy: 0.2398 - val_loss: 191.3707 - val_accuracy: 0.4444\n",
            "Epoch 5/7\n",
            "86/86 [==============================] - 8s 95ms/step - loss: 1.4292 - accuracy: 0.2924 - val_loss: 129.3690 - val_accuracy: 0.4444\n",
            "Epoch 6/7\n",
            "86/86 [==============================] - 8s 95ms/step - loss: 1.4952 - accuracy: 0.2398 - val_loss: 129.4275 - val_accuracy: 0.2222\n",
            "Epoch 7/7\n",
            "86/86 [==============================] - 8s 95ms/step - loss: 1.4606 - accuracy: 0.2632 - val_loss: 101.1790 - val_accuracy: 0.1111\n"
          ]
        }
      ],
      "source": [
        "history1_1  = Res50_BiLSTM_1.fit(x, y, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fba749",
      "metadata": {},
      "outputs": [],
      "source": [
        "history1_2  = Res50_BiLSTM_1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db41abca",
      "metadata": {
        "id": "db41abca",
        "outputId": "b395283c-9eb1-47b9-8e3a-519ec1fe06d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34/34 [==============================] - 163s 5s/step - loss: 1.4542 - accuracy: 0.4118 - val_loss: 0.7121 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "# history1  = hybrid_model1.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly\n",
        "# history1_2  = hybrid_model1_1.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks = [checkpoint])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "c08074a8",
      "metadata": {
        "id": "c08074a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.366071</td>\n",
              "      <td>0.873563</td>\n",
              "      <td>0.659441</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.385803</td>\n",
              "      <td>0.867816</td>\n",
              "      <td>0.377790</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.306944</td>\n",
              "      <td>0.890805</td>\n",
              "      <td>0.681669</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.341706</td>\n",
              "      <td>0.850575</td>\n",
              "      <td>0.655428</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.281064</td>\n",
              "      <td>0.936782</td>\n",
              "      <td>0.409325</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  accuracy  val_loss  val_accuracy\n",
              "0  0.366071  0.873563  0.659441      0.833333\n",
              "1  0.385803  0.867816  0.377790      0.833333\n",
              "2  0.306944  0.890805  0.681669      0.833333\n",
              "3  0.341706  0.850575  0.655428      0.666667\n",
              "4  0.281064  0.936782  0.409325      0.833333"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df = pd.DataFrame(history.history)\n",
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4638a03e",
      "metadata": {},
      "source": [
        "### CNN + BiLSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18416963",
      "metadata": {
        "id": "18416963"
      },
      "source": [
        "#### CNN + BiLSTM (224 x 224 images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c3d4d370",
      "metadata": {
        "id": "c3d4d370"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aafca8d0",
      "metadata": {
        "id": "aafca8d0",
        "outputId": "69140757-e710-49d7-bf32-f2d2b2773fe4"
      },
      "outputs": [],
      "source": [
        "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
        "cnn_model = models.Sequential()\n",
        "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "cnn_model.add(layers.MaxPooling2D()) #(2,2)\n",
        "cnn_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn_model.add(layers.MaxPooling2D())\n",
        "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn_model.add(layers.MaxPooling2D())\n",
        "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "actions = len(subset_video_count.keys())\n",
        "\n",
        "# Define the BiLSTM model\n",
        "bilstm_model = Sequential()\n",
        "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
        "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
        "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
        "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
        "\n",
        "# Additional layers\n",
        "bilstm_model.add(layers.Flatten())  # new added line -> (Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor.)\n",
        "bilstm_model.add(Dense(64, activation='relu'))\n",
        "bilstm_model.add(Dense(32, activation='relu'))\n",
        "bilstm_model.add(Dense(actions, activation = 'softmax'))\n",
        "\n",
        "# Combine the ResNet50 and BiLSTM models\n",
        "combined_input = Input(shape=(20, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
        "cnn_output = TimeDistributed(cnn_model)(combined_input)\n",
        "cnn_output = TimeDistributed(GlobalAveragePooling2D())(cnn_model)  # Adjust pooling layer based on your requirements\n",
        "# resnet_output = Reshape((60, -1))(resnet_output)\n",
        "# print(resnet_output.shape)\n",
        "bilstm_output = bilstm_model(cnn_output)\n",
        "\n",
        "# ---------------\n",
        "\n",
        "\n",
        "# Concatenate the outputs\n",
        "# merged = concatenate([resnet_output, bilstm_output])\n",
        "\n",
        "# Additional layers for combined model\n",
        "# final_output = Dense(128, activation='relu')(merged)\n",
        "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
        "\n",
        "# Create the final model\n",
        "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
        "\n",
        "# ----------------\n",
        "hybrid_model2 = Model(inputs=combined_input, outputs=bilstm_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9465718a",
      "metadata": {
        "id": "9465718a"
      },
      "outputs": [],
      "source": [
        "def build_cnn(shape=(224, 224, 3)):\n",
        "    cnn_model = models.Sequential()\n",
        "    cnn_model.add(Conv2D(128, (3, 3), activation='relu', input_shape=shape))\n",
        "    cnn_model.add(BatchNormalization(momentum=0.9))\n",
        "    cnn_model.add(MaxPooling2D((2,2), strides=2)) #(2,2)\n",
        "    cnn_model.add(Conv2D(256, (3, 3), activation='relu', strides=1))\n",
        "    cnn_model.add(BatchNormalization(momentum=0.9))\n",
        "    cnn_model.add(MaxPooling2D((2,2), strides=2))\n",
        "    cnn_model.add(Conv2D(128, (3, 3), activation='relu', strides=1))\n",
        "    cnn_model.add(BatchNormalization(momentum=0.9))\n",
        "    cnn_model.add(MaxPooling2D((2,2), strides=2))\n",
        "    cnn_model.add(Conv2D(32, (3, 3), activation='relu', strides=1))\n",
        "    # output = GlobalMaxPool2D()\n",
        "\n",
        "    return cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "8d49b596",
      "metadata": {
        "id": "8d49b596"
      },
      "outputs": [],
      "source": [
        "def build_BiLSTM1(noActions, shape = (20, 2048)):\n",
        "    # cnn_model = build_cnn(shape[1:])\n",
        "\n",
        "    # Define the BiLSTM model\n",
        "    bilstm_model = Sequential()\n",
        "    # bilstm_model.add(TimeDistributed(first_model)(inputShape)) #(20, 2048)\n",
        "    bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, input_shape=shape))) #, input_shape=(60, 2048)\n",
        "    bilstm_model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "    bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "    bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
        "\n",
        "    # Additional layers\n",
        "    bilstm_model.add(Dense(128, activation='relu'))\n",
        "    bilstm_model.add(Dropout(.5))\n",
        "    bilstm_model.add(Dense(64, activation='relu'))\n",
        "    bilstm_model.add(Dropout(.5))\n",
        "    bilstm_model.add(Dense(32, activation='relu'))\n",
        "    bilstm_model.add(Dense(noActions, activation = 'softmax'))\n",
        "\n",
        "    return bilstm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "ba3b3955",
      "metadata": {},
      "outputs": [],
      "source": [
        "def CNN_BiLSTM(inputShape=(20,224,224,3), noActions=len(subset_video_count.keys())):\n",
        "    combined_input = Input(shape=inputShape)  # Assuming image shape is (224, 224, 3)\n",
        "    cnnModel = build_cnn()\n",
        "    cnn_output = TimeDistributed(cnnModel)(combined_input)\n",
        "    cnn_output = TimeDistributed(GlobalMaxPool2D())(cnn_output)\n",
        "    bilstm_model = build_BiLSTM1(noActions, (20, 2048))\n",
        "    bilstm_output = bilstm_model(cnn_output)\n",
        "    hybrid_model2 = Model(inputs=combined_input, outputs=bilstm_output)\n",
        "    return hybrid_model2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e7adfc",
      "metadata": {},
      "source": [
        "#### CNN Model (Alone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "baf017d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input layer for the video frames\n",
        "input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "cnn_model = build_cnn()\n",
        "# Apply TimeDistributed to the ResNet50 base\n",
        "cnn_output = TimeDistributed(cnn_model)(input_frames)\n",
        "\n",
        "# Global Average Pooling layer to reduce spatial dimensions\n",
        "global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(cnn_output)\n",
        "\n",
        "# # Fully connected layers for video-level classification\n",
        "# dense_layer = (Flatten())(global_avg_pool)\n",
        "# dense_layer = Dense(64, activation='relu')(dense_layer)\n",
        "# output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "dense_layer = (Flatten())(global_avg_pool)\n",
        "dense_layer = (Dense(128, activation='relu'))(dense_layer)\n",
        "dense_layer = (Dropout(.3)) (dense_layer)\n",
        "dense_layer = (Dense(64, activation='relu'))(dense_layer)\n",
        "dense_layer = (Dropout(.3)) (dense_layer)\n",
        "dense_layer = (Dense(32, activation='relu'))(dense_layer)\n",
        "output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "\n",
        "# Create the final model\n",
        "hybrid_model3 = tf.keras.models.Model(inputs=input_frames, outputs=output_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f545ed7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "hybrid_model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6843f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history2_1 = hybrid_model3.fit(x, y, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks = [checkpoint])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3c1c20a5",
      "metadata": {
        "id": "3c1c20a5"
      },
      "outputs": [],
      "source": [
        "CNN_BiLSTM_1 = CNN_BiLSTM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "776d8cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "92cf88d4",
      "metadata": {
        "id": "92cf88d4"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "CNN_BiLSTM_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "952cc5ce",
      "metadata": {
        "id": "952cc5ce",
        "outputId": "394243b6-3823-4817-f3d5-79e01845e7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "86/86 [==============================] - 27s 182ms/step - loss: 1.3986 - accuracy: 0.2632 - val_loss: 1.4303 - val_accuracy: 0.2222\n",
            "Epoch 2/5\n",
            "86/86 [==============================] - 13s 152ms/step - loss: 1.4294 - accuracy: 0.2339 - val_loss: 1.4040 - val_accuracy: 0.2222\n",
            "Epoch 3/5\n",
            "86/86 [==============================] - 13s 153ms/step - loss: 1.3968 - accuracy: 0.2690 - val_loss: 1.3923 - val_accuracy: 0.2222\n",
            "Epoch 4/5\n",
            "86/86 [==============================] - 13s 153ms/step - loss: 1.4022 - accuracy: 0.2339 - val_loss: 1.3916 - val_accuracy: 0.1111\n",
            "Epoch 5/5\n",
            "86/86 [==============================] - 13s 153ms/step - loss: 1.4086 - accuracy: 0.2164 - val_loss: 1.3916 - val_accuracy: 0.1111\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history2_1 = CNN_BiLSTM_1.fit(x, y, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7c27fa",
      "metadata": {
        "id": "ec7c27fa"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history2 = hybrid_model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc749b03",
      "metadata": {},
      "source": [
        "### Xception + BiLSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb46aa3d",
      "metadata": {
        "id": "bb46aa3d"
      },
      "source": [
        "#### Xception + BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0139f7",
      "metadata": {
        "id": "0e0139f7"
      },
      "outputs": [],
      "source": [
        "# Assuming you have a ResNet50 model loaded with pre-trained weights\n",
        "Xnet_model=tf.keras.applications.xception.Xception(weights='imagenet',include_top=False, input_shape = (224, 224, 3))\n",
        "Xnet_model.trainable = False\n",
        "\n",
        "actions = len(subset_video_count.keys())\n",
        "\n",
        "# Define the BiLSTM model\n",
        "bilstm_model = Sequential()\n",
        "# bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation = 'relu', input_shape = (60,2048))))\n",
        "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=True, activation='relu'), input_shape=(60, 2048)))\n",
        "bilstm_model.add(Bidirectional(LSTM(32, return_sequences=False, activation = 'relu')))\n",
        "# bilstm_model.add(Bidirectional(LSTM(units=128, return_sequences=True), input_shape=(60, 2048)))  # Adjust feature_size based on your data\n",
        "\n",
        "# Additional layers\n",
        "# bilstm_model.add(layers.Flatten())  # new added line -> (Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor.)\n",
        "bilstm_model.add(Dense(64, activation='relu'))\n",
        "bilstm_model.add(Dense(32, activation='relu'))\n",
        "bilstm_model.add(Dense(actions, activation = 'softmax'))\n",
        "\n",
        "# Combine the ResNet50 and BiLSTM models\n",
        "combined_input = Input(shape=(60, 224, 224, 3))  # Assuming image shape is (224, 224, 3)\n",
        "Xnet_output = TimeDistributed(Xnet_model)(combined_input)\n",
        "Xnet_output = TimeDistributed(GlobalAveragePooling2D())(Xnet_model)  # Adjust pooling layer based on your requirements\n",
        "# resnet_output = Reshape((60, -1))(resnet_output)\n",
        "# print(resnet_output.shape)\n",
        "bilstm_output = bilstm_model(Xnet_output)\n",
        "\n",
        "# ---------------\n",
        "\n",
        "\n",
        "# Concatenate the outputs\n",
        "# merged = concatenate([resnet_output, bilstm_output])\n",
        "\n",
        "# Additional layers for combined model\n",
        "# final_output = Dense(128, activation='relu')(merged)\n",
        "# final_output = Dense(Dense(actions.shape[0], activation='softmax')(final_output)  # Adjust num_classes based on your task\n",
        "\n",
        "# Create the final model\n",
        "# hybrid_model = Model(inputs=combined_input, outputs=final_output)\n",
        "\n",
        "# ----------------\n",
        "hybrid_model3 = Model(inputs=combined_input, outputs=bilstm_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3796679a",
      "metadata": {
        "id": "3796679a"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "hybrid_model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9175aa",
      "metadata": {
        "id": "dd9175aa",
        "outputId": "6ee48cfb-e6bf-43b1-cf78-c601331b7800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 60, 224, 224, 3   0         \n",
            "                             )]                                  \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 60, 7, 7, 2048)    23587712  \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 60, 2048)          0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 3)                 1129411   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24717123 (94.29 MB)\n",
            "Trainable params: 1129411 (4.31 MB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Display model summary\n",
        "hybrid_model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c69ec6",
      "metadata": {
        "id": "62c69ec6"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "# frames = 60\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b1b702",
      "metadata": {
        "id": "c3b1b702",
        "outputId": "9ec68887-b8ef-4811-fd71-7732630cf989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 2981.3291 - accuracy: 0.3625WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "40/40 [==============================] - 264s 6s/step - loss: 2981.3291 - accuracy: 0.3625 - val_loss: 3343.4873 - val_accuracy: 0.4444\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 10839.7988 - accuracy: 0.2375WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "40/40 [==============================] - 246s 6s/step - loss: 10839.7988 - accuracy: 0.2375 - val_loss: 35102.9688 - val_accuracy: 0.3333\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 38513.8633 - accuracy: 0.3125WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "40/40 [==============================] - 246s 6s/step - loss: 38513.8633 - accuracy: 0.3125 - val_loss: 29871.4375 - val_accuracy: 0.3333\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 19742.9902 - accuracy: 0.3625WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "40/40 [==============================] - 245s 6s/step - loss: 19742.9902 - accuracy: 0.3625 - val_loss: 101186.1094 - val_accuracy: 0.1111\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - ETA: 0s - loss: 43741.2734 - accuracy: 0.3875WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "40/40 [==============================] - 249s 6s/step - loss: 43741.2734 - accuracy: 0.3875 - val_loss: 18177.5293 - val_accuracy: 0.3333\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1c38e59f6d0>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "history1  = hybrid_model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CR4rIK_NBDvG",
      "metadata": {
        "id": "CR4rIK_NBDvG"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history1  = hybrid_model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks = [checkpoint, earlyStopping])  # Adjust epochs and batch_size accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953ac727",
      "metadata": {},
      "source": [
        "#### Xception Model (Alone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e6bf20d2",
      "metadata": {
        "id": "e6bf20d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 6s 0us/step\n",
            "83697664/83683744 [==============================] - 6s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained ResNet50 without the top (fully connected) layers\n",
        "# resnet_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "Xnet_model=tf.keras.applications.Xception(weights='imagenet',include_top=False, input_shape = (224, 224, 3))\n",
        "\n",
        "# Freeze the ResNet50 layers\n",
        "for layer in Xnet_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Input layer for the video frames\n",
        "input_frames = Input(shape=(20, 224, 224, 3))\n",
        "\n",
        "# Apply TimeDistributed to the ResNet50 base\n",
        "Xnet_output = TimeDistributed(Xnet_model)(input_frames)\n",
        "\n",
        "# Global Average Pooling layer to reduce spatial dimensions\n",
        "global_avg_pool = TimeDistributed(GlobalAveragePooling2D())(Xnet_output)\n",
        "\n",
        "# Fully connected layers for video-level classification\n",
        "dense_layer = (Flatten())(global_avg_pool)\n",
        "dense_layer = Dense(512, activation='relu')(dense_layer)\n",
        "output_layer = Dense(4, activation='softmax')(dense_layer)  # Assuming 4 classes, adjust as needed\n",
        "\n",
        "# Create the final model\n",
        "hybrid_model3 = tf.keras.models.Model(inputs=input_frames, outputs=output_layer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "25a322f0",
      "metadata": {
        "id": "25a322f0"
      },
      "outputs": [],
      "source": [
        "# Compile the model (you can customize the optimizer, loss, and metrics)\n",
        "hybrid_model3.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "171cdc8c",
      "metadata": {
        "id": "171cdc8c",
        "outputId": "58635ac0-81d7-46fd-9114-71faa504bfb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "86/86 [==============================] - 23s 112ms/step - loss: 13.3291 - accuracy: 0.5116 - val_loss: 0.7728 - val_accuracy: 0.7500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dr2007\\.conda\\envs\\test_env_gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  warnings.warn('Custom mask layers require a config and must override '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20\n",
            "86/86 [==============================] - 8s 97ms/step - loss: 0.9065 - accuracy: 0.8953 - val_loss: 5.4353 - val_accuracy: 0.2500\n",
            "Epoch 3/20\n",
            "86/86 [==============================] - 8s 96ms/step - loss: 0.6338 - accuracy: 0.9419 - val_loss: 0.2168 - val_accuracy: 0.8750\n",
            "Epoch 4/20\n",
            "86/86 [==============================] - 8s 97ms/step - loss: 0.3683 - accuracy: 0.9535 - val_loss: 0.5950 - val_accuracy: 0.8750\n",
            "Epoch 5/20\n",
            "86/86 [==============================] - 8s 97ms/step - loss: 0.6044 - accuracy: 0.9535 - val_loss: 0.3805 - val_accuracy: 0.8750\n",
            "Epoch 6/20\n",
            "86/86 [==============================] - 8s 97ms/step - loss: 0.0312 - accuracy: 0.9884 - val_loss: 1.2743 - val_accuracy: 0.8750\n",
            "Epoch 7/20\n",
            "86/86 [==============================] - 8s 97ms/step - loss: 0.5081 - accuracy: 0.9360 - val_loss: 0.1155 - val_accuracy: 0.8750\n",
            "Epoch 8/20\n",
            "86/86 [==============================] - 8s 98ms/step - loss: 0.1817 - accuracy: 0.9767 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "86/86 [==============================] - 8s 98ms/step - loss: 1.4480e-04 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "86/86 [==============================] - 9s 99ms/step - loss: 2.4533e-05 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "86/86 [==============================] - 8s 99ms/step - loss: 1.7874e-05 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "86/86 [==============================] - 9s 99ms/step - loss: 1.4043e-05 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "86/86 [==============================] - 9s 99ms/step - loss: 1.1638e-05 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "86/86 [==============================] - 9s 100ms/step - loss: 9.9304e-06 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "86/86 [==============================] - 9s 101ms/step - loss: 8.6760e-06 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "86/86 [==============================] - 9s 100ms/step - loss: 7.6119e-06 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "86/86 [==============================] - 9s 100ms/step - loss: 6.7868e-06 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "86/86 [==============================] - 9s 100ms/step - loss: 6.0815e-06 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "86/86 [==============================] - 9s 100ms/step - loss: 5.5106e-06 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "86/86 [==============================] - 9s 99ms/step - loss: 5.0277e-06 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "history3_1 = hybrid_model3.fit(x, y, epochs=20, batch_size=2, validation_data=(X_val, y_val), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b7f79a",
      "metadata": {},
      "outputs": [],
      "source": [
        "history3_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b31773",
      "metadata": {
        "id": "02b31773"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834621b1",
      "metadata": {
        "id": "834621b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ce2bbe96",
      "metadata": {
        "id": "ce2bbe96"
      },
      "source": [
        "## Monitoring the Model's performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ec95797d",
      "metadata": {
        "id": "ec95797d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "43f9c37f",
      "metadata": {
        "id": "43f9c37f"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(history3_1.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fa10d5ff",
      "metadata": {
        "id": "fa10d5ff",
        "outputId": "0e5fe132-66c0-4336-aa6a-fc24738a4d9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13.329148</td>\n",
              "      <td>0.511628</td>\n",
              "      <td>0.772833</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.906504</td>\n",
              "      <td>0.895349</td>\n",
              "      <td>5.435304</td>\n",
              "      <td>0.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.633750</td>\n",
              "      <td>0.941860</td>\n",
              "      <td>0.216787</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.368266</td>\n",
              "      <td>0.953488</td>\n",
              "      <td>0.594998</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.604407</td>\n",
              "      <td>0.953488</td>\n",
              "      <td>0.380530</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.031164</td>\n",
              "      <td>0.988372</td>\n",
              "      <td>1.274274</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.508103</td>\n",
              "      <td>0.936047</td>\n",
              "      <td>0.115542</td>\n",
              "      <td>0.875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.181713</td>\n",
              "      <td>0.976744</td>\n",
              "      <td>0.023413</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000145</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.022204</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000025</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.021647</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000018</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000014</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.021549</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000012</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.021108</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.020480</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000009</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.019802</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.019523</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000007</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.018674</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.018193</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.017693</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.017196</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  accuracy  val_loss  val_accuracy\n",
              "0   13.329148  0.511628  0.772833         0.750\n",
              "1    0.906504  0.895349  5.435304         0.250\n",
              "2    0.633750  0.941860  0.216787         0.875\n",
              "3    0.368266  0.953488  0.594998         0.875\n",
              "4    0.604407  0.953488  0.380530         0.875\n",
              "5    0.031164  0.988372  1.274274         0.875\n",
              "6    0.508103  0.936047  0.115542         0.875\n",
              "7    0.181713  0.976744  0.023413         1.000\n",
              "8    0.000145  1.000000  0.022204         1.000\n",
              "9    0.000025  1.000000  0.021647         1.000\n",
              "10   0.000018  1.000000  0.021873         1.000\n",
              "11   0.000014  1.000000  0.021549         1.000\n",
              "12   0.000012  1.000000  0.021108         1.000\n",
              "13   0.000010  1.000000  0.020480         1.000\n",
              "14   0.000009  1.000000  0.019802         1.000\n",
              "15   0.000008  1.000000  0.019523         1.000\n",
              "16   0.000007  1.000000  0.018674         1.000\n",
              "17   0.000006  1.000000  0.018193         1.000\n",
              "18   0.000006  1.000000  0.017693         1.000\n",
              "19   0.000005  1.000000  0.017196         1.000"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3d920073",
      "metadata": {
        "id": "3d920073",
        "outputId": "10023754-2948-40e0-bc2d-de8b508ba108"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# comparing the training and validation loss.\n",
        "metrics_df[[\"loss\",\"val_loss\"]].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "5e9e158b",
      "metadata": {
        "id": "5e9e158b",
        "outputId": "91734bab-51a6-419d-cfa5-45dc19243c9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# comparing the training and validation accuracy.\n",
        "metrics_df[[\"accuracy\",\"val_accuracy\"]].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6fa78f",
      "metadata": {
        "id": "da6fa78f"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b275fe2",
      "metadata": {
        "id": "4b275fe2",
        "outputId": "d3aa4d81-4eeb-4631-9f01-783b57b3270a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 24s 24s/step - loss: 17.1240 - accuracy: 0.5000\n",
            "Accuracy on validation dataset: 0.5\n",
            "Loss on validation dataset: 17.123992919921875\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_accuracy = hybrid_model1.evaluate(X_val, y_val)\n",
        "print('Accuracy on validation dataset:', val_accuracy)\n",
        "print('Loss on validation dataset:', val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4064807f",
      "metadata": {
        "id": "4064807f"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy on test dataset:', val_accuracy)\n",
        "print('Loss on test dataset:', val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d38f9e",
      "metadata": {
        "id": "08d38f9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe17e75",
      "metadata": {
        "id": "bbe17e75"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25186f60",
      "metadata": {
        "id": "25186f60"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "13f5a983",
        "e195f4da",
        "b6ae7098",
        "086f9e04",
        "0dc1285b",
        "3b725def",
        "d4526c39",
        "61a303a8",
        "944047ec",
        "26bac943",
        "687f20f4",
        "7aK5lyRKAB6c",
        "9cd7e79b"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
